{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. –ü–µ—Ä—Å–æ–Ω–∞ ‚Äî —á–µ–ª–æ–∫–µ/–ø–µ—Ä—Å–æ–Ω–∞–∂, —É–ø–æ–º–∏–Ω–∞–µ–º—ã–π –≤ —Ç–µ–∫—Å—Ç–µ (–ù–∞–ø—Ä–∏–º–µ—Ä: –ù–∞—Å—Ç—è –ò–≤–ª–µ–µ–≤–∞, –î—è–¥—è –§–µ–¥–æ—Ä, –ë–∞—Å—Ç–∞, –ï–≤–≥–µ–Ω–∏–π –û–Ω–µ–≥–∏–Ω). –ü–µ—Ä—Å–æ–Ω–∞ - —Ç–æ–ª—å–∫–æ –∏–º—è —á–µ–ª–æ–≤–µ–∫–∞/–ø–µ—Ä—Å–æ–Ω–∞–∂–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, —Ñ—Ä–∞–∑–∞ \"–†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å –æ—Ç–¥–µ–ª–∞\" –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø–µ—Ä—Å–æ–Ω–æ–π.\n",
    "\n",
    "2. –õ–æ–∫–∞—Ü–∏—è ‚Äî –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, —Ç–∞–∫–∏–µ –∫–∞–∫ —Å—Ç—Ä–∞–Ω–∞, –≥–æ—Ä–æ–¥, –∫—Ä–∞–π, –æ–±–ª–∞—Å—Ç—å, –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ (–ù–∞–ø—Ä–∏–º–µ—Ä: –ú–æ—Å–∫–≤–∞, –†–æ—Å—Å–∏—è, –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω, –Ω–∞ –§–µ—Å—Ç–∏–≤–∞–ª—å–Ω–æ–π —É–ª–∏—Ü–µ, –≥–æ—Å—Ç–∏—Ü–∞ –°–æ–ª–Ω–µ—á–Ω–∞—è)\n",
    "\n",
    "3. –î–∞—Ç–∞ ‚Äî –¥–∞—Ç–∞ —Å–æ–±—ã—Ç–∏—è (–ù–∞–ø—Ä–∏–º–µ—Ä: 4 —è–Ω–≤–∞—Ä—è, –≤ 1992 –≥–æ–¥—É, –≤ –æ–∫—Ç—è–±—Ä–µ)\n",
    "\n",
    "4. –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è ‚Äî –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è, –æ—Ä–≥–∞–Ω –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–π –≤–ª–∞—Å—Ç–∏, —É—á—Ä–µ–∂–¥–µ–Ω–∏–µ (–ù–∞–ø—Ä–∏–º–µ—Ä: –Ø–Ω–¥–µ–∫—Å, –ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è, –°–æ–≤–µ—Ç –§–µ–¥–µ—Ä–∞—Ü–∏–∏, –û–û–û \"–†–æ–≥–∞ –∏ –ö–æ–ø—ã—Ç–∞\")\n",
    "\n",
    "5. –ë—Ä–µ–Ω–¥ - –±—Ä–µ–Ω–¥ (–ù–∞–ø—Ä–∏–º–µ—Ä:  Samsung, Audi, Toyota, )\n",
    "\n",
    "6. –ú–æ–¥–µ–ª—å - –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–ù–∞–ø—Ä–∏–º–µ—Ä: Galaxy S10, RS6, Corolla)\n",
    "\n",
    "7. –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ - –Ω–∞–∑–≤–∞–Ω–∏–µ —à–æ—É, —Å–µ—Ä–∏–∞–ª–∞, —Ñ–∏–ª—å–º–∞, –ø—Ä–æ–µ–∫—Ç–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ (–ù–∞–ø—Ä–∏–º–µ—Ä: –ë–∏—Ç–≤–∞ —ç–∫—Å—Ç—Ä–∞—Å–µ–Ω—Å–æ–≤, –±–µ—Ä–µ–º–µ–Ω–Ω–∞ –≤ 16, –ò–Ω—Ç–µ—Ä–Ω—ã, –õ–∏–¥–µ—Ä—ã –†–æ—Å—Å–∏–∏, –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω–∞—è –∫–æ–æ–ø–µ—Ä–∞—Ü–∏—è –∏ —ç–∫—Å–ø–æ—Ä—Ç)\n",
    "\n",
    "8. –°–µ–∑–æ–Ω - —Å–µ–∑–æ–Ω/—á–∞—Å—Ç—å —Å–µ—Ä–∏–∞–ª–∞ (–ù–∞–ø—Ä–∏–º–µ—Ä: —Ç—Ä–µ—Ç–∏–π, –ø–µ—Ä–≤—ã–π, 1)\n",
    "\n",
    "9. –°–µ—Ä–∏—è - —Å–µ—Ä–∏—è —Å–µ—Ä–∏–∞–ª–∞, –ø–µ—Ä–µ–¥–∞—á–∏ (–ù–∞–ø—Ä–∏–º–µ—Ä: –ø–µ—Ä–≤–∞—è, –∑–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–∞—è, 4)\n",
    "\n",
    "10. –õ–∏–≥–∞ - –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–ø–æ—Ä—Ç–∏–≤–Ω–æ–π –ª–∏–≥–∏ (–ù–∞–ø—Ä–∏–º–µ—Ä: –ß–µ–º–ø–∏–æ–Ω–∞—Ç –ï–≤—Ä–æ–ø—ã-2024, –õ–∞–ª–∏–≥–∞, –ê–Ω–≥–ª–∏–π—Å–∫–∞—è –ø—Ä–µ–º—å–µ—Ä –ª–∏–≥–∞, –í–æ—Å—Ç–æ—á–Ω–æ-–µ–≤—Ä–æ–ø–µ–π—Å–∫–∞—è —Ö–æ–∫–∫–µ–π–Ω–∞—è –ª–∏–≥–∞, —á–µ–º–ø–∏–æ–Ω–∞—Ç–µ –º–∏—Ä–∞)\n",
    "\n",
    "11. –ö–æ–º–∞–Ω–¥–∞ - –∫–æ–º–∞–Ω–¥–∞ (—Å–ø–æ—Ä—Ç–∏–≤–Ω–∞—è, —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–∞—è) (–ù–∞–ø—Ä–∏–º–µ—Ä: –¶–°–ö–ê, Manchester United, –ö–ê–ú–ê–ó-–º–∞—Å—Ç–µ—Ä)\n",
    "\n",
    "12. –í–∏–¥ —Å–ø–æ—Ä—Ç–∞ - –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–ø–æ—Ä—Ç–∞ –∏–ª–∏ –ø–æ–¥—Ç–∏–ø–∞ (–ù–∞–ø—Ä–∏–º–µ—Ä: —Ñ—É—Ç–±–æ–ª, –∫–µ—Ä–ª–∏–Ω–≥, –∞–≤—Ç–æ—Å–ø–æ—Ä—Ç, —Ç–∞–Ω–≥–æ)\n",
    "\n",
    "13. –í–∏–¥–µ–æ–∏–≥—Ä–∞ ‚Äî –Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ–∏–≥—Ä—ã (–ù–∞–ø—Ä–∏–º–µ—Ä: GTA, Call of Duty, –ú–∞–π–Ω–∫—Ä–∞—Ñ—Ç)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–¥—ã –ø—Ä–æ–µ–∫—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers evaluate  install accelerate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76521/1682736822.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import razdel\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ê–≥–µ–Ω—Ç 117: –ò–∑ –ê—Ñ—Ä–∏–∫–∏ —Å –ª—é–±–æ–≤—å—é ‚Äî –†...</td>\n",
       "      <td>{\"label\":\"–ª–æ–∫–∞—Ü–∏—è\"\\,\"offset\":26\\,\"length\":6\\,\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ö–æ–ª–µ–Ω–≤–∞–ª –ò–Ω—Ñ–∏–Ω–∏—Ç–∏ –ö—É –∏–∫—Å 56= 5.6 V...</td>\n",
       "      <td>{\"label\":\"–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è\"\\,\"offset\":196\\,\"length\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –í–´–ó–û–í –î–ï–ú–û–ù–ê = –í—ã–∑–≤–∞–ª –°–µ—Ä–æ–≥–æ –ß–µ–ª–æ–≤...</td>\n",
       "      <td>{\"label\":\"–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\"\\,\"offset\":12\\,\"len...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –î–æ–≤–æ–µ–Ω–Ω–∞—è –Ω–µ–º–µ—Ü–∫–∞—è –∫–∏—Ä—Ö–∞ –≤ –ö–∞–ª–∏–Ω–∏–Ω...</td>\n",
       "      <td>{\"label\":\"–Ω–µ –Ω–∞–π–¥–µ–Ω–æ\"\\,\"offset\":162\\,\"length\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; \"–°–ø–∞—Ä—Ç–∞–∫—É\" –ø–æ–º–æ–≥–ª–∏ —Å—É–¥—å–∏? –õ–æ–∫–æ–º–æ—Ç–∏...</td>\n",
       "      <td>{\"label\":\"–∫–æ–º–∞–Ω–¥–∞\"\\,\"offset\":13\\,\"length\":8\\,\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_info  \\\n",
       "0  <–ù–ê–ó–í–ê–ù–ò–ï:> –ê–≥–µ–Ω—Ç 117: –ò–∑ –ê—Ñ—Ä–∏–∫–∏ —Å –ª—é–±–æ–≤—å—é ‚Äî –†...   \n",
       "1  <–ù–ê–ó–í–ê–ù–ò–ï:> –ö–æ–ª–µ–Ω–≤–∞–ª –ò–Ω—Ñ–∏–Ω–∏—Ç–∏ –ö—É –∏–∫—Å 56= 5.6 V...   \n",
       "2  <–ù–ê–ó–í–ê–ù–ò–ï:> –í–´–ó–û–í –î–ï–ú–û–ù–ê = –í—ã–∑–≤–∞–ª –°–µ—Ä–æ–≥–æ –ß–µ–ª–æ–≤...   \n",
       "3  <–ù–ê–ó–í–ê–ù–ò–ï:> –î–æ–≤–æ–µ–Ω–Ω–∞—è –Ω–µ–º–µ—Ü–∫–∞—è –∫–∏—Ä—Ö–∞ –≤ –ö–∞–ª–∏–Ω–∏–Ω...   \n",
       "4  <–ù–ê–ó–í–ê–ù–ò–ï:> \"–°–ø–∞—Ä—Ç–∞–∫—É\" –ø–æ–º–æ–≥–ª–∏ —Å—É–¥—å–∏? –õ–æ–∫–æ–º–æ—Ç–∏...   \n",
       "\n",
       "                                            entities  \n",
       "0  {\"label\":\"–ª–æ–∫–∞—Ü–∏—è\"\\,\"offset\":26\\,\"length\":6\\,\"...  \n",
       "1  {\"label\":\"–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è\"\\,\"offset\":196\\,\"length\"...  \n",
       "2  {\"label\":\"–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\"\\,\"offset\":12\\,\"len...  \n",
       "3  {\"label\":\"–Ω–µ –Ω–∞–π–¥–µ–Ω–æ\"\\,\"offset\":162\\,\"length\":...  \n",
       "4  {\"label\":\"–∫–æ–º–∞–Ω–¥–∞\"\\,\"offset\":13\\,\"length\":8\\,\"...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"ner_data_train.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ê–≥–µ–Ω—Ç 117: –ò–∑ –ê—Ñ—Ä–∏–∫–∏ —Å –ª—é–±–æ–≤—å—é ‚Äî –†...</td>\n",
       "      <td>[{'label': '–ª–æ–∫–∞—Ü–∏—è', 'offset': 26, 'length': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ö–æ–ª–µ–Ω–≤–∞–ª –ò–Ω—Ñ–∏–Ω–∏—Ç–∏ –ö—É –∏–∫—Å 56= 5.6 V...</td>\n",
       "      <td>[{'label': '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è', 'offset': 196, 'leng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –í–´–ó–û–í –î–ï–ú–û–ù–ê = –í—ã–∑–≤–∞–ª –°–µ—Ä–æ–≥–æ –ß–µ–ª–æ–≤...</td>\n",
       "      <td>[{'label': '–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞', 'offset': 12, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –î–æ–≤–æ–µ–Ω–Ω–∞—è –Ω–µ–º–µ—Ü–∫–∞—è –∫–∏—Ä—Ö–∞ –≤ –ö–∞–ª–∏–Ω–∏–Ω...</td>\n",
       "      <td>[{'label': '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ', 'offset': 162, 'lengt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; \"–°–ø–∞—Ä—Ç–∞–∫—É\" –ø–æ–º–æ–≥–ª–∏ —Å—É–¥—å–∏? –õ–æ–∫–æ–º–æ—Ç–∏...</td>\n",
       "      <td>[{'label': '–∫–æ–º–∞–Ω–¥–∞', 'offset': 13, 'length': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_info  \\\n",
       "0  <–ù–ê–ó–í–ê–ù–ò–ï:> –ê–≥–µ–Ω—Ç 117: –ò–∑ –ê—Ñ—Ä–∏–∫–∏ —Å –ª—é–±–æ–≤—å—é ‚Äî –†...   \n",
       "1  <–ù–ê–ó–í–ê–ù–ò–ï:> –ö–æ–ª–µ–Ω–≤–∞–ª –ò–Ω—Ñ–∏–Ω–∏—Ç–∏ –ö—É –∏–∫—Å 56= 5.6 V...   \n",
       "2  <–ù–ê–ó–í–ê–ù–ò–ï:> –í–´–ó–û–í –î–ï–ú–û–ù–ê = –í—ã–∑–≤–∞–ª –°–µ—Ä–æ–≥–æ –ß–µ–ª–æ–≤...   \n",
       "3  <–ù–ê–ó–í–ê–ù–ò–ï:> –î–æ–≤–æ–µ–Ω–Ω–∞—è –Ω–µ–º–µ—Ü–∫–∞—è –∫–∏—Ä—Ö–∞ –≤ –ö–∞–ª–∏–Ω–∏–Ω...   \n",
       "4  <–ù–ê–ó–í–ê–ù–ò–ï:> \"–°–ø–∞—Ä—Ç–∞–∫—É\" –ø–æ–º–æ–≥–ª–∏ —Å—É–¥—å–∏? –õ–æ–∫–æ–º–æ—Ç–∏...   \n",
       "\n",
       "                                            entities  \n",
       "0  [{'label': '–ª–æ–∫–∞—Ü–∏—è', 'offset': 26, 'length': ...  \n",
       "1  [{'label': '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è', 'offset': 196, 'leng...  \n",
       "2  [{'label': '–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞', 'offset': 12, '...  \n",
       "3  [{'label': '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ', 'offset': 162, 'lengt...  \n",
       "4  [{'label': '–∫–æ–º–∞–Ω–¥–∞', 'offset': 13, 'length': ...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –¥–∞–Ω–Ω—ã–µ —Å–ø–∞—Ä—Å–µ–Ω—ã —Å –¢–æ–ª–æ–∫–∏, –ø–æ—ç—Ç–æ–º—É –º–æ–≥—É—Ç –∏–º–µ—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å —Å–∏–º–≤–æ–ª–∞–º–∏ –∏ –∏—Ö –Ω—É–∂–Ω–æ –∏–∑–±–µ–∂–∞—Ç—å, \n",
    "# —É–¥–∞–ª–∏—Ç—å –ª–∏—à–Ω–∏–µ '\\' –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏–∑ str –≤ —Å–ø–∏—Å–æ–∫ dict-–æ–≤\n",
    "df = data.copy()\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\,', ',')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\\\\\\\', '\\\\')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: '[' + l + ']'if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: json.loads(l)if isinstance(l, str) else l)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ–ø–µ—Ä—å –∏–∑ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞–º –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ (—Ç–æ–∫–µ–Ω–∞) –µ–≥–æ\n",
    "# —Ç–µ–≥ (label) –∏–∑ —Ä–∞–∑–º–µ—Ç–∫–∏, —á—Ç–æ–±—ã –ø–æ—Ç–æ–º –ø—Ä–µ–¥–∞—Ç—å –≤ –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "\n",
    "def extract_labels(item):\n",
    "    \n",
    "    # –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —É–¥–æ–±–Ω—ã–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ razdel, \n",
    "    # –æ–Ω–∞ –ø–æ–º–∏–º–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —Å–ª–æ–≤–∞, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–∞–∂–Ω—ã–µ\n",
    "    # –¥–ª—è –Ω–∞—Å —á–∏—Å–ª–∞ - –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü —Å–ª–æ–≤–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö\n",
    "    \n",
    "    raw_toks = list(razdel.tokenize(item['video_info']))\n",
    "    words = [tok.text for tok in raw_toks]\n",
    "    # –ø—Ä–∏—Å–≤–æ–∏–º –¥–ª—è –Ω–∞—á–∞–ª–∞ –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É —Ç–µ–≥ '–û' - —Ç–µ–≥, –æ–∑–Ω–∞—á–∞—é—â–∏–π –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ NER-–∞\n",
    "    word_labels = ['O'] * len(raw_toks)\n",
    "    char2word = [None] * len(item['video_info'])\n",
    "    # —Ç–∞–∫ –∫–∞–∫ NER –º–æ–∂–µ–º —Å–æ—Å—Ç–∞—è—Ç—å –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–≤, —Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —ç—Ç—É –∏–Ω—Ñ–æ—Ä—Ü–∏—é\n",
    "    for i, word in enumerate(raw_toks):\n",
    "        char2word[word.start:word.stop] = [i] * len(word.text)\n",
    "\n",
    "    labels = item['entities']\n",
    "    if isinstance(labels, dict):\n",
    "        labels = [labels]\n",
    "    if labels is not None:\n",
    "        for e in labels:\n",
    "            if e['label'] != '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ':\n",
    "                e_words = sorted({idx for idx in char2word[e['offset']:e['offset']+e['length']] if idx is not None})\n",
    "                if e_words:\n",
    "                    word_labels[e_words[0]] = 'B-' + e['label']\n",
    "                    for idx in e_words[1:]:\n",
    "                        word_labels[idx] = 'I-' + e['label']\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        return {'tokens': words, 'tags': word_labels}\n",
    "    else: return {'tokens': words, 'tags': word_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_labels(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making train ant test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data = [extract_labels(item) for i, item in df.iterrows()]\n",
    "ner_train, ner_test = train_test_split(ner_data, test_size=0.01, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-–î–∞—Ç–∞',\n",
       " 'B-–±—Ä–µ–Ω–¥',\n",
       " 'B-–≤–∏–¥ —Å–ø–æ—Ä—Ç–∞',\n",
       " 'B-–≤–∏–¥–µ–æ–∏–≥—Ä–∞',\n",
       " 'B-–∫–æ–º–∞–Ω–¥–∞',\n",
       " 'B-–ª–∏–≥–∞',\n",
       " 'B-–ª–æ–∫–∞—Ü–∏—è',\n",
       " 'B-–º–æ–¥–µ–ª—å',\n",
       " 'B-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞',\n",
       " 'B-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è',\n",
       " 'B-–ø–µ—Ä—Å–æ–Ω–∞',\n",
       " 'B-—Å–µ–∑–æ–Ω',\n",
       " 'B-—Å–µ—Ä–∏—è',\n",
       " 'I-–î–∞—Ç–∞',\n",
       " 'I-–±—Ä–µ–Ω–¥',\n",
       " 'I-–≤–∏–¥ —Å–ø–æ—Ä—Ç–∞',\n",
       " 'I-–≤–∏–¥–µ–æ–∏–≥—Ä–∞',\n",
       " 'I-–∫–æ–º–∞–Ω–¥–∞',\n",
       " 'I-–ª–∏–≥–∞',\n",
       " 'I-–ª–æ–∫–∞—Ü–∏—è',\n",
       " 'I-–º–æ–¥–µ–ª—å',\n",
       " 'I-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞',\n",
       " 'I-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è',\n",
       " 'I-–ø–µ—Ä—Å–æ–Ω–∞',\n",
       " 'I-—Å–µ–∑–æ–Ω',\n",
       " 'I-—Å–µ—Ä–∏—è']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = sorted({label for item in ner_train for label in item['tags']})\n",
    "if 'O' in label_list:\n",
    "    label_list.remove('O')\n",
    "    label_list = ['O'] + label_list\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 6357\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 65\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
    "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
    "})\n",
    "ner_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ú–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"xlm-roberta-large-finetuned-conll03-english\"\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, device=device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –£ Bert —Å–≤–æ–π —Å–æ–±—Å–≤–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Å–ª–æ–≤–∞ –Ω–∞ –º–µ–ª–∫–∏–µ —Ç–æ–∫–µ–Ω—ã, –ø–æ—ç—Ç–æ–º—É –Ω–∞–º –Ω—É–∂–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å–≤—É—é—â–∏–µ –∏–º –Ω–µ—Ä—ã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b771467445b749ae99ff9e8a6703c1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3496e0dc67423abdc9b39a008d59e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä–∏–∫ —Å–æ–æ—Ç–≤–µ—Å—Ç–≤–∏—è —Ç–µ–≥–∞ –∏ –µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "model.config.id2label = dict(enumerate(label_list))\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = torch.nn.Linear(1024, len(label_list), bias=True).cuda()\n",
    "model.num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç—Ä–∏–∫ –≤–æ–∑—å–º–µ–º precision, recall, accuracy, –¥–ª—è —ç—Ç–æ–≥–æ –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ–¥ Ner –∑–∞–¥–∞—á—É –±–∏–±–ª–∏–æ—Ç–µ–∫—É seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76521/152412463.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    "    output_dir=\"models/xlm_roberta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"test\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.trainer import logger as noisy_logger\n",
    "# noisy_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–µ—Ä—Ç–∞ –º–æ–∂–Ω–æ —ç–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –∑–∞–º–æ—Ä–æ–∑–∫–æ–π/—Ä–∞–∑–º–æ—Ä–æ–∑–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–µ–≤, –∑–¥–µ—Å—å –º—ã –æ—Å—Ç–∞–≤–∏–º –≤—Å–µ —Å–ª–æ–∏ —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–º–∏ \n",
    "# –î–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å –≤—Å—é –±–µ—Ä—Ç–æ–≤—É—é —á–∞—Å—Ç—å, –∫—Ä–æ–º–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞, –Ω–æ —Ç–æ–≥–¥–∞ –∫–∞—á–µ—Å–≤—Ç–æ –±—É–¥–µ—Ç –ø–æ—Ö—É–∂–µ\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='584' max='805' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [584/805 16:45 < 06:21, 0.58 it/s, Epoch 3.62/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.312765</td>\n",
       "      <td>0.534657</td>\n",
       "      <td>0.619244</td>\n",
       "      <td>0.573850</td>\n",
       "      <td>0.911907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.303146</td>\n",
       "      <td>0.530329</td>\n",
       "      <td>0.597361</td>\n",
       "      <td>0.561853</td>\n",
       "      <td>0.909396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.315811</td>\n",
       "      <td>0.521158</td>\n",
       "      <td>0.639558</td>\n",
       "      <td>0.574320</td>\n",
       "      <td>0.909185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/hacks_ai/XLM_Roberta_Training.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f736176656c6f7632227d@ssh-remote%2B172.16.204.14/workspace/hacks_ai/XLM_Roberta_Training.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_that_works/lib/python3.11/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_that_works/lib/python3.11/site-packages/transformers/trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_that_works/lib/python3.11/site-packages/transformers/trainer.py:2787\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2786\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2787\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2789\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_that_works/lib/python3.11/site-packages/accelerate/accelerator.py:1989\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1988\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1989\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_that_works/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_that_works/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/xlm_roberta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.40144357085227966,\n",
       " 'eval_precision': 0.5195944744175016,\n",
       " 'eval_recall': 0.5724737082761774,\n",
       " 'eval_f1': 0.5447538538040775,\n",
       " 'eval_accuracy': 0.905239093160602}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –¢–µ—Å—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ—Å—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(sum(true_labels, []), sum(true_predictions, []), labels=label_list),\n",
    "    index=label_list,\n",
    "    columns=label_list\n",
    ")\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1222036/1682736822.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import razdel\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ê–≥–µ–Ω—Ç 117: –ò–∑ –ê—Ñ—Ä–∏–∫–∏ —Å –ª—é–±–æ–≤—å—é ‚Äî –†...</td>\n",
       "      <td>{\"label\":\"–ª–æ–∫–∞—Ü–∏—è\"\\,\"offset\":26\\,\"length\":6\\,\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ö–æ–ª–µ–Ω–≤–∞–ª –ò–Ω—Ñ–∏–Ω–∏—Ç–∏ –ö—É –∏–∫—Å 56= 5.6 V...</td>\n",
       "      <td>{\"label\":\"–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è\"\\,\"offset\":196\\,\"length\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –í–´–ó–û–í –î–ï–ú–û–ù–ê = –í—ã–∑–≤–∞–ª –°–µ—Ä–æ–≥–æ –ß–µ–ª–æ–≤...</td>\n",
       "      <td>{\"label\":\"–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\"\\,\"offset\":12\\,\"len...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –î–æ–≤–æ–µ–Ω–Ω–∞—è –Ω–µ–º–µ—Ü–∫–∞—è –∫–∏—Ä—Ö–∞ –≤ –ö–∞–ª–∏–Ω–∏–Ω...</td>\n",
       "      <td>{\"label\":\"–Ω–µ –Ω–∞–π–¥–µ–Ω–æ\"\\,\"offset\":162\\,\"length\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; \"–°–ø–∞—Ä—Ç–∞–∫—É\" –ø–æ–º–æ–≥–ª–∏ —Å—É–¥—å–∏? –õ–æ–∫–æ–º–æ—Ç–∏...</td>\n",
       "      <td>{\"label\":\"–∫–æ–º–∞–Ω–¥–∞\"\\,\"offset\":13\\,\"length\":8\\,\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_info  \\\n",
       "0  <–ù–ê–ó–í–ê–ù–ò–ï:> –ê–≥–µ–Ω—Ç 117: –ò–∑ –ê—Ñ—Ä–∏–∫–∏ —Å –ª—é–±–æ–≤—å—é ‚Äî –†...   \n",
       "1  <–ù–ê–ó–í–ê–ù–ò–ï:> –ö–æ–ª–µ–Ω–≤–∞–ª –ò–Ω—Ñ–∏–Ω–∏—Ç–∏ –ö—É –∏–∫—Å 56= 5.6 V...   \n",
       "2  <–ù–ê–ó–í–ê–ù–ò–ï:> –í–´–ó–û–í –î–ï–ú–û–ù–ê = –í—ã–∑–≤–∞–ª –°–µ—Ä–æ–≥–æ –ß–µ–ª–æ–≤...   \n",
       "3  <–ù–ê–ó–í–ê–ù–ò–ï:> –î–æ–≤–æ–µ–Ω–Ω–∞—è –Ω–µ–º–µ—Ü–∫–∞—è –∫–∏—Ä—Ö–∞ –≤ –ö–∞–ª–∏–Ω–∏–Ω...   \n",
       "4  <–ù–ê–ó–í–ê–ù–ò–ï:> \"–°–ø–∞—Ä—Ç–∞–∫—É\" –ø–æ–º–æ–≥–ª–∏ —Å—É–¥—å–∏? –õ–æ–∫–æ–º–æ—Ç–∏...   \n",
       "\n",
       "                                            entities  \n",
       "0  {\"label\":\"–ª–æ–∫–∞—Ü–∏—è\"\\,\"offset\":26\\,\"length\":6\\,\"...  \n",
       "1  {\"label\":\"–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è\"\\,\"offset\":196\\,\"length\"...  \n",
       "2  {\"label\":\"–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\"\\,\"offset\":12\\,\"len...  \n",
       "3  {\"label\":\"–Ω–µ –Ω–∞–π–¥–µ–Ω–æ\"\\,\"offset\":162\\,\"length\":...  \n",
       "4  {\"label\":\"–∫–æ–º–∞–Ω–¥–∞\"\\,\"offset\":13\\,\"length\":8\\,\"...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"ner_data_train.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ê–≥–µ–Ω—Ç 117: –ò–∑ –ê—Ñ—Ä–∏–∫–∏ —Å –ª—é–±–æ–≤—å—é ‚Äî –†...</td>\n",
       "      <td>[{'label': '–ª–æ–∫–∞—Ü–∏—è', 'offset': 26, 'length': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ö–æ–ª–µ–Ω–≤–∞–ª –ò–Ω—Ñ–∏–Ω–∏—Ç–∏ –ö—É –∏–∫—Å 56= 5.6 V...</td>\n",
       "      <td>[{'label': '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è', 'offset': 196, 'leng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –í–´–ó–û–í –î–ï–ú–û–ù–ê = –í—ã–∑–≤–∞–ª –°–µ—Ä–æ–≥–æ –ß–µ–ª–æ–≤...</td>\n",
       "      <td>[{'label': '–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞', 'offset': 12, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –î–æ–≤–æ–µ–Ω–Ω–∞—è –Ω–µ–º–µ—Ü–∫–∞—è –∫–∏—Ä—Ö–∞ –≤ –ö–∞–ª–∏–Ω–∏–Ω...</td>\n",
       "      <td>[{'label': '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ', 'offset': 162, 'lengt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; \"–°–ø–∞—Ä—Ç–∞–∫—É\" –ø–æ–º–æ–≥–ª–∏ —Å—É–¥—å–∏? –õ–æ–∫–æ–º–æ—Ç–∏...</td>\n",
       "      <td>[{'label': '–∫–æ–º–∞–Ω–¥–∞', 'offset': 13, 'length': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_info  \\\n",
       "0  <–ù–ê–ó–í–ê–ù–ò–ï:> –ê–≥–µ–Ω—Ç 117: –ò–∑ –ê—Ñ—Ä–∏–∫–∏ —Å –ª—é–±–æ–≤—å—é ‚Äî –†...   \n",
       "1  <–ù–ê–ó–í–ê–ù–ò–ï:> –ö–æ–ª–µ–Ω–≤–∞–ª –ò–Ω—Ñ–∏–Ω–∏—Ç–∏ –ö—É –∏–∫—Å 56= 5.6 V...   \n",
       "2  <–ù–ê–ó–í–ê–ù–ò–ï:> –í–´–ó–û–í –î–ï–ú–û–ù–ê = –í—ã–∑–≤–∞–ª –°–µ—Ä–æ–≥–æ –ß–µ–ª–æ–≤...   \n",
       "3  <–ù–ê–ó–í–ê–ù–ò–ï:> –î–æ–≤–æ–µ–Ω–Ω–∞—è –Ω–µ–º–µ—Ü–∫–∞—è –∫–∏—Ä—Ö–∞ –≤ –ö–∞–ª–∏–Ω–∏–Ω...   \n",
       "4  <–ù–ê–ó–í–ê–ù–ò–ï:> \"–°–ø–∞—Ä—Ç–∞–∫—É\" –ø–æ–º–æ–≥–ª–∏ —Å—É–¥—å–∏? –õ–æ–∫–æ–º–æ—Ç–∏...   \n",
       "\n",
       "                                            entities  \n",
       "0  [{'label': '–ª–æ–∫–∞—Ü–∏—è', 'offset': 26, 'length': ...  \n",
       "1  [{'label': '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è', 'offset': 196, 'leng...  \n",
       "2  [{'label': '–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞', 'offset': 12, '...  \n",
       "3  [{'label': '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ', 'offset': 162, 'lengt...  \n",
       "4  [{'label': '–∫–æ–º–∞–Ω–¥–∞', 'offset': 13, 'length': ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –¥–∞–Ω–Ω—ã–µ —Å–ø–∞—Ä—Å–µ–Ω—ã —Å –¢–æ–ª–æ–∫–∏, –ø–æ—ç—Ç–æ–º—É –º–æ–≥—É—Ç –∏–º–µ—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å —Å–∏–º–≤–æ–ª–∞–º–∏ –∏ –∏—Ö –Ω—É–∂–Ω–æ –∏–∑–±–µ–∂–∞—Ç—å, \n",
    "# —É–¥–∞–ª–∏—Ç—å –ª–∏—à–Ω–∏–µ '\\' –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏–∑ str –≤ —Å–ø–∏—Å–æ–∫ dict-–æ–≤\n",
    "df = data.copy()\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\,', ',')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\\\\\\\', '\\\\')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: '[' + l + ']'if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: json.loads(l)if isinstance(l, str) else l)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–µ–ø–µ—Ä—å –∏–∑ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞–º –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ (—Ç–æ–∫–µ–Ω–∞) –µ–≥–æ\n",
    "# —Ç–µ–≥ (label) –∏–∑ —Ä–∞–∑–º–µ—Ç–∫–∏, —á—Ç–æ–±—ã –ø–æ—Ç–æ–º –ø—Ä–µ–¥–∞—Ç—å –≤ –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "\n",
    "def extract_labels(item):\n",
    "    \n",
    "    # –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —É–¥–æ–±–Ω—ã–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ razdel, \n",
    "    # –æ–Ω–∞ –ø–æ–º–∏–º–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —Å–ª–æ–≤–∞, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–∞–∂–Ω—ã–µ\n",
    "    # –¥–ª—è –Ω–∞—Å —á–∏—Å–ª–∞ - –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü —Å–ª–æ–≤–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö\n",
    "    \n",
    "    raw_toks = list(razdel.tokenize(item['video_info']))\n",
    "    words = [tok.text for tok in raw_toks]\n",
    "    # –ø—Ä–∏—Å–≤–æ–∏–º –¥–ª—è –Ω–∞—á–∞–ª–∞ –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É —Ç–µ–≥ '–û' - —Ç–µ–≥, –æ–∑–Ω–∞—á–∞—é—â–∏–π –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ NER-–∞\n",
    "    word_labels = ['O'] * len(raw_toks)\n",
    "    char2word = [None] * len(item['video_info'])\n",
    "    # —Ç–∞–∫ –∫–∞–∫ NER –º–æ–∂–µ–º —Å–æ—Å—Ç–∞—è—Ç—å –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–≤, —Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —ç—Ç—É –∏–Ω—Ñ–æ—Ä—Ü–∏—é\n",
    "    for i, word in enumerate(raw_toks):\n",
    "        char2word[word.start:word.stop] = [i] * len(word.text)\n",
    "\n",
    "    labels = item['entities']\n",
    "    if isinstance(labels, dict):\n",
    "        labels = [labels]\n",
    "    if labels is not None:\n",
    "        for e in labels:\n",
    "            if e['label'] != '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ':\n",
    "                e_words = sorted({idx for idx in char2word[e['offset']:e['offset']+e['length']] if idx is not None})\n",
    "                if e_words:\n",
    "                    word_labels[e_words[0]] = 'B-' + e['label']\n",
    "                    for idx in e_words[1:]:\n",
    "                        word_labels[idx] = 'I-' + e['label']\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        return {'tokens': words, 'tags': word_labels}\n",
    "    else: return {'tokens': words, 'tags': word_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_labels(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making train ant test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data = [extract_labels(item) for i, item in df.iterrows()]\n",
    "ner_train, ner_test = train_test_split(ner_data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-–î–∞—Ç–∞',\n",
       " 'B-–±—Ä–µ–Ω–¥',\n",
       " 'B-–≤–∏–¥ —Å–ø–æ—Ä—Ç–∞',\n",
       " 'B-–≤–∏–¥–µ–æ–∏–≥—Ä–∞',\n",
       " 'B-–∫–æ–º–∞–Ω–¥–∞',\n",
       " 'B-–ª–∏–≥–∞',\n",
       " 'B-–ª–æ–∫–∞—Ü–∏—è',\n",
       " 'B-–º–æ–¥–µ–ª—å',\n",
       " 'B-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞',\n",
       " 'B-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è',\n",
       " 'B-–ø–µ—Ä—Å–æ–Ω–∞',\n",
       " 'B-—Å–µ–∑–æ–Ω',\n",
       " 'B-—Å–µ—Ä–∏—è',\n",
       " 'I-–î–∞—Ç–∞',\n",
       " 'I-–±—Ä–µ–Ω–¥',\n",
       " 'I-–≤–∏–¥ —Å–ø–æ—Ä—Ç–∞',\n",
       " 'I-–≤–∏–¥–µ–æ–∏–≥—Ä–∞',\n",
       " 'I-–∫–æ–º–∞–Ω–¥–∞',\n",
       " 'I-–ª–∏–≥–∞',\n",
       " 'I-–ª–æ–∫–∞—Ü–∏—è',\n",
       " 'I-–º–æ–¥–µ–ª—å',\n",
       " 'I-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞',\n",
       " 'I-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è',\n",
       " 'I-–ø–µ—Ä—Å–æ–Ω–∞',\n",
       " 'I-—Å–µ–∑–æ–Ω',\n",
       " 'I-—Å–µ—Ä–∏—è']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = sorted({label for item in ner_train for label in item['tags']})\n",
    "if 'O' in label_list:\n",
    "    label_list.remove('O')\n",
    "    label_list = ['O'] + label_list\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 6357\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 65\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
    "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
    "})\n",
    "ner_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ú–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"xlm-roberta-large\"\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, device=device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –£ Bert —Å–≤–æ–π —Å–æ–±—Å–≤–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Å–ª–æ–≤–∞ –Ω–∞ –º–µ–ª–∫–∏–µ —Ç–æ–∫–µ–Ω—ã, –ø–æ—ç—Ç–æ–º—É –Ω–∞–º –Ω—É–∂–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–∫–µ–Ω—ã –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å–≤—É—é—â–∏–µ –∏–º –Ω–µ—Ä—ã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8989f3e7be41afb3ec3aa1c9e36a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039e8623cc504bf190bf8ff5e28c6941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä–∏–∫ —Å–æ–æ—Ç–≤–µ—Å—Ç–≤–∏—è —Ç–µ–≥–∞ –∏ –µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "model.config.id2label = dict(enumerate(label_list))\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = torch.nn.Linear(1024, len(label_list), bias=True).cuda()\n",
    "model.num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í –∫–∞—á–µ—Å—Ç–≤–µ –º–µ—Ç—Ä–∏–∫ –≤–æ–∑—å–º–µ–º precision, recall, accuracy, –¥–ª—è —ç—Ç–æ–≥–æ –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–æ–¥ Ner –∑–∞–¥–∞—á—É –±–∏–±–ª–∏–æ—Ç–µ–∫—É seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    "    output_dir=\"models/xlm_roberta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"test\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.trainer import logger as noisy_logger\n",
    "# noisy_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –±–µ—Ä—Ç–∞ –º–æ–∂–Ω–æ —ç–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å –∑–∞–º–æ—Ä–æ–∑–∫–æ–π/—Ä–∞–∑–º–æ—Ä–æ–∑–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–µ–≤, –∑–¥–µ—Å—å –º—ã –æ—Å—Ç–∞–≤–∏–º –≤—Å–µ —Å–ª–æ–∏ —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–º–∏ \n",
    "# –î–ª—è –±—ã—Å—Ç—Ä–æ—Ç—ã –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –∑–∞–º–æ—Ä–æ–∑–∏—Ç—å –≤—Å—é –±–µ—Ä—Ç–æ–≤—É—é —á–∞—Å—Ç—å, –∫—Ä–æ–º–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞, –Ω–æ —Ç–æ–≥–¥–∞ –∫–∞—á–µ—Å–≤—Ç–æ –±—É–¥–µ—Ç –ø–æ—Ö—É–∂–µ\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='161' max='161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [161/161 04:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.332525</td>\n",
       "      <td>0.525924</td>\n",
       "      <td>0.627474</td>\n",
       "      <td>0.572229</td>\n",
       "      <td>0.909663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=161, training_loss=0.198312332911521, metrics={'train_runtime': 282.9288, 'train_samples_per_second': 18.157, 'train_steps_per_second': 0.569, 'total_flos': 3367775178141534.0, 'train_loss': 0.198312332911521, 'epoch': 1.0})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/xlm_roberta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.40144357085227966,\n",
       " 'eval_precision': 0.5195944744175016,\n",
       " 'eval_recall': 0.5724737082761774,\n",
       " 'eval_f1': 0.5447538538040775,\n",
       " 'eval_accuracy': 0.905239093160602}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –¢–µ—Å—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ—Å—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
    "\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(sum(true_labels, []), sum(true_predictions, []), labels=label_list),\n",
    "    index=label_list,\n",
    "    columns=label_list\n",
    ")\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='ner', aggregation_strategy='average', device='cuda')\n",
    "\n",
    "def predict_ner(text, tokenizer, model, pipe, verbose=True):\n",
    "    tokens = tokenizer(text, truncation=True, is_split_into_words=True, return_tensors='pt')\n",
    "    tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(**tokens)\n",
    "    # print(pred.logits.shape)\n",
    "    indices = pred.logits.argmax(dim=-1)[0].cpu().numpy()\n",
    "    token_text = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "    labels = []\n",
    "    for t, idx in zip(token_text, indices):\n",
    "        if '##' not in t:\n",
    "            labels.append(label_list[idx])\n",
    "        if verbose:\n",
    "            print(f'{t:15s} {label_list[idx]:10s}')\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ë–û–ï–¶ –ë–ï–ó –ü–†–ê–í–ò–õ, –¢–†–ï–ô–õ–ï–† –Ω–∞ —Ä—É—Å—Å–∫–æ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –®–û–ö! –ö–ê–ö –ñ–ï –≠–¢–û–ú–£ –ë–û–ú–ñ–£ –í–ï–ó–ï–¢ –í Br...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –î–†–ï–í–ù–ò–ï –†–û–ë–û–¢–´ NATASHA TALON –ò LEO...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ñ–ï–ù–ê –ü–£–¢–ï–®–ï–°–¢–í–ï–ù–ù–ò–ö–ê –í–û –í–†–ï–ú–ï–ù–ò = ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ö–∏–Ω–æ–∑–∞–ª –î–ö –ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç –Ω–∞ –º—É–ª—å—Ç—Ñ–∏–ª—å...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_info  entities_prediction\n",
       "0  <–ù–ê–ó–í–ê–ù–ò–ï:> –ë–û–ï–¶ –ë–ï–ó –ü–†–ê–í–ò–õ, –¢–†–ï–ô–õ–ï–† –Ω–∞ —Ä—É—Å—Å–∫–æ...                  NaN\n",
       "1  <–ù–ê–ó–í–ê–ù–ò–ï:> –®–û–ö! –ö–ê–ö –ñ–ï –≠–¢–û–ú–£ –ë–û–ú–ñ–£ –í–ï–ó–ï–¢ –í Br...                  NaN\n",
       "2  <–ù–ê–ó–í–ê–ù–ò–ï:> –î–†–ï–í–ù–ò–ï –†–û–ë–û–¢–´ NATASHA TALON –ò LEO...                  NaN\n",
       "3  <–ù–ê–ó–í–ê–ù–ò–ï:> –ñ–ï–ù–ê –ü–£–¢–ï–®–ï–°–¢–í–ï–ù–ù–ò–ö–ê –í–û –í–†–ï–ú–ï–ù–ò = ...                  NaN\n",
       "4  <–ù–ê–ó–í–ê–ù–ò–ï:> –ö–∏–Ω–æ–∑–∞–ª –î–ö –ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç –Ω–∞ –º—É–ª—å—Ç—Ñ–∏–ª—å...                  NaN"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"ner_data_test.csv\")\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1606, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¥–∞–Ω–Ω—ã–µ —Å–ø–∞—Ä—Å–µ–Ω—ã —Å –¢–æ–ª–æ–∫–∏, –ø–æ—ç—Ç–æ–º—É –º–æ–≥—É—Ç –∏–º–µ—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å —Å–∏–º–≤–æ–ª–∞–º–∏ –∏ –∏—Ö –Ω—É–∂–Ω–æ –∏–∑–±–µ–∂–∞—Ç—å, \n",
    "# —É–¥–∞–ª–∏—Ç—å –ª–∏—à–Ω–∏–µ '\\' –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏–∑ str –≤ —Å–ø–∏—Å–æ–∫ dict-–æ–≤\n",
    "# test = data.copy()\n",
    "# test['entities'] = test['entities'].apply(lambda l: l.replace('\\,', ',')if isinstance(l, str) else l)\n",
    "# test['entities'] = test['entities'].apply(lambda l: l.replace('\\\\\\\\', '\\\\')if isinstance(l, str) else l)\n",
    "# test['entities'] = test['entities'].apply(lambda l: '[' + l + ']'if isinstance(l, str) else l)\n",
    "# test['entities'] = test['entities'].apply(lambda l: json.loads(l)if isinstance(l, str) else l)\n",
    "\n",
    "# test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# submission = pd.DataFrame(columns=[['video_info', 'entities_prediction']])\n",
    "# submission['entities_prediction'] = submission['entities_prediction'].astype('object')\n",
    "\n",
    "# def sample_submission(text, tokenizer, model, pipe, submission):\n",
    "#     for i, elem in tqdm(enumerate(ner_test.iloc[:100])):\n",
    "#         labels = predict_ner(elem['tokens'], tokenizer, model, pipe, verbose=False)\n",
    "#         submission.loc[i, 'video_info'] = elem[\"tokens\"]\n",
    "\n",
    "#         submission.loc[i, 'entities_prediction'] = [[label] for label in labels]\n",
    "#     return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1150363ac054614ba884dbb4697e8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_len = sample[\"entities_prediction\"].apply(lambda x: len(eval(x)))\n",
    "\n",
    "for i, elem in tqdm(enumerate(test[\"video_info\"]), total=len(test[\"video_info\"])):\n",
    "    raw_toks = list(tokenize(elem))\n",
    "    words = [tok.text for tok in raw_toks]\n",
    "    \n",
    "    labels = predict_ner(words, tokenizer, model, pipe, verbose=False)\n",
    "    # submission.loc[i, 'video_info'] = elem[\"tokens\"]\n",
    "    \n",
    "    labels = labels[1:-1]\n",
    "        \n",
    "    if len(labels) < sample_len[i]:\n",
    "        amount = sample_len[i] - len(labels)\n",
    "        labels += ['O'] * amount\n",
    "        print(amount)\n",
    "    \n",
    "    labels = labels[:sample_len[i]]\n",
    "    \n",
    "    if len(labels) != sample_len[i]:\n",
    "        print(\"—á—Ç–æ\")\n",
    "        # labels = labels[:len(words)]\n",
    "\n",
    "    # test.loc[i, 'entities_prediction'] = [[label] for label in labels]\n",
    "    test.loc[i, 'entities_prediction'] = [[label] for label in labels]\n",
    "# return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 112)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([tok.text for tok in list(tokenize(test[\"video_info\"][0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.loc[0][\"entities_prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\t\tO\n",
      "–ù–ê–ó–í–ê–ù–ò–ï\t\tO\n",
      ":\t\tO\n",
      ">\t\tO\n",
      "–ë–û–ï–¶\t\tO\n",
      "–ë–ï–ó\t\tO\n",
      "–ü–†–ê–í–ò–õ\t\tO\n",
      ",\t\tB-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "–¢–†–ï–ô–õ–ï–†\t\tB-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "–Ω–∞\t\tB-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "—Ä—É—Å—Å–∫–æ–º\t\tI-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      ",\t\tI-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "—Ñ–∏–ª—å–º\t\tI-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "2021\t\tO\n",
      "|\t\tO\n",
      "–±–æ–µ–≤–∏–∫\t\tO\n",
      ",\t\tO\n",
      "MMA\t\tO\n",
      "<\t\tO\n",
      "–û–ü–ò–°–ê–ù–ò–ï\t\tO\n",
      ":\t\tO\n",
      ">\t\tO\n",
      "–ë–æ–ª—å—à–µ\t\tO\n",
      "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\t\tO\n",
      "–≤\t\tB-–î–∞—Ç–∞\n",
      "–Ω–∞—à–µ–π\t\tO\n",
      "–≥—Ä—É–ø–ø–µ\t\tO\n",
      "–í–ö\t\tO\n",
      ":\t\tO\n",
      "<\t\tO\n",
      "LINK\t\tO\n",
      ">\t\tO\n",
      "–†—É—Å—Å–∫–∏–π\t\tB-–≤–∏–¥ —Å–ø–æ—Ä—Ç–∞\n",
      "—Ç—Ä–µ–π–ª–µ—Ä\t\tB-–≤–∏–¥ —Å–ø–æ—Ä—Ç–∞\n",
      "—Ñ–∏–ª—å–º–∞\t\tO\n",
      "–ë–û–ï–¶\t\tO\n",
      "–ë–ï–ó\t\tO\n",
      "–ü–†–ê–í–ò–õ\t\tO\n",
      "2021\t\tO\n",
      "–≥\t\tO\n",
      "üé¨\t\tO\n",
      "–ë–æ–µ—Ü\t\tO\n",
      "–±–µ–∑\t\tO\n",
      "–ø—Ä–∞–≤–∏–ª\t\tO\n",
      "(\t\tO\n",
      "2021\t\tO\n",
      ")\t\tO\n",
      "—Ä—É—Å—Å–∫–∏–π\t\tO\n",
      "—Ç—Ä–µ–π–ª–µ—Ä\t\tO\n",
      "Notorious\t\tO\n",
      "Nick\t\tO\n",
      "(\t\tO\n",
      "2021\t\tO\n",
      ")\t\tO\n",
      "/\t\tO\n",
      "–°–®–ê\t\tO\n",
      "–¥—Ä–∞–º–∞\t\tO\n",
      ",\t\tO\n",
      "–±–æ–µ–≤–∏–∫\t\tO\n",
      "–î–∞—Ç–∞\t\tO\n",
      "–≤—ã—Ö–æ–¥–∞\t\tB-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      ":\t\tB-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "7\t\tB-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "–∞–≤–≥—É—Å—Ç–∞\t\tI-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "2021\t\tI-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "–≥\t\tI-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "(\t\tB-–î–∞—Ç–∞\n",
      "–º–∏—Ä\t\tI-–î–∞—Ç–∞\n",
      ")\t\tO\n",
      "2\t\tO\n",
      "—Å–µ–Ω—Ç—è–±—Ä—è\t\tB-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "2021\t\tB-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "–≥\t\tI-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "(\t\tI-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "–†–æ—Å—Å–∏—è\t\tO\n",
      ")\t\tB-–î–∞—Ç–∞\n",
      "–ù–∏–∫\t\tO\n",
      "–ù—å—é—ç–ª–ª\t\tO\n",
      ",\t\tO\n",
      "–æ–¥–Ω–æ—Ä—É–∫–∏–π\t\tO\n",
      "–±–æ–µ—Ü\t\tO\n",
      "–ú–ú–ê\t\tB-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      ",\t\tB-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "–ø–æ–ª—É—á–∞–µ—Ç\t\tB-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "—Ä–µ–¥–∫—É—é\t\tI-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞\n",
      "–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å\t\tO\n",
      "–ø—Ä–∏–Ω—è—Ç—å\t\tB-–î–∞—Ç–∞\n",
      "—É—á–∞—Å—Ç–∏–µ\t\tO\n",
      "–≤\t\tO\n",
      "—á–µ–º–ø–∏–æ–Ω–∞—Ç–µ\t\tB-–ª–æ–∫–∞—Ü–∏—è\n",
      ".\t\tO\n",
      "–ì–µ—Ä–æ–π\t\tO\n",
      "–∂–∞–∂–¥–µ—Ç\t\tO\n",
      "–ø–æ–±–µ–¥–∏—Ç—å\t\tO\n",
      "–Ω–µ\t\tO\n",
      "—Ç–æ–ª—å–∫–æ\t\tO\n",
      "—Ä–∞–¥–∏\t\tO\n",
      "—Å–µ–±—è\t\tO\n",
      ",\t\tO\n",
      "–Ω–æ\t\tB-–î–∞—Ç–∞\n",
      "–∏\t\tI-–î–∞—Ç–∞\n",
      "—Ä–∞–¥–∏\t\tI-–î–∞—Ç–∞\n",
      "–ª—é–¥–µ–π\t\tI-–î–∞—Ç–∞\n",
      "—Å–æ\t\tO\n",
      "–≤—Å–µ–≥–æ\t\tO\n",
      "–º–∏—Ä–∞\t\tO\n",
      ",\t\tB-–î–∞—Ç–∞\n",
      "–∫–æ—Ç–æ—Ä—ã–º\t\tI-–î–∞—Ç–∞\n",
      "–∑–Ω–∞–∫–æ–º—ã\t\tI-–î–∞—Ç–∞\n",
      "—Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ\t\tI-–î–∞—Ç–∞\n",
      "—Ç—Ä—É–¥–Ω–æ—Å—Ç–∏\t\tO\n",
      ".\t\tB-–ª–æ–∫–∞—Ü–∏—è\n",
      "–†–µ–∂–∏—Å—Å–µ—Ä\t\tO\n",
      ":\t\tB-–ø–µ—Ä—Å–æ–Ω–∞\n",
      "–ê–∞—Ä–æ–Ω\t\tI-–ø–µ—Ä—Å–æ–Ω–∞\n",
      "–õ–µ–æ–Ω–≥\t\tI-–ø–µ—Ä—Å–æ–Ω–∞\n",
      "–ê–∫—Ç—ë—Ä—ã\t\tI-–ø–µ—Ä—Å–æ–Ω–∞\n",
      ":\t\tO\n",
      "–≠–ª–∏–∑–∞–±–µ—Ç\t\tO\n",
      "–†—ë–º\t\tO\n",
      ",\t\tO\n",
      "–ö–µ–≤–∏–Ω\t\tO\n",
      "–ü–æ–ª–ª–∞–∫\t\tO\n",
      ",\t\tO\n",
      "–ö–æ–¥–∏\t\tB-–≤–∏–¥ —Å–ø–æ—Ä—Ç–∞\n",
      "–ö—Ä–∏—Å—Ç–∏–∞–Ω\t\tB-–≤–∏–¥ —Å–ø–æ—Ä—Ç–∞\n",
      ",\t\tO\n",
      "–ë—ç—Ä—Ä–∏\t\tO\n",
      "–õ–∏–≤–∏–Ω–≥—Å—Ç–æ–Ω\t\tO\n",
      ",\t\tO\n",
      "–≠–Ω—Ç–æ–Ω–∏\t\tO\n",
      "–°–Ω–æ—É\t\tO\n",
      ",\t\tO\n",
      "–°—ç–º—é—ç–ª\t\tO\n",
      "–ò–≤—ç–Ω\t\tO\n",
      "–•–æ—Ä–æ–≤–∏—Ü\t\tO\n",
      ",\t\tO\n",
      "–ö–æ–Ω–ª–∞–Ω\t\tO\n",
      "–ö–∏—Å–∏–ª–µ–≤–∏—á\t\tO\n",
      ",\t\tO\n",
      "–ú–∞—Ä–∫\t\tO\n",
      "–°\t\tO\n",
      ".\t\tO\n",
      "–ê–ª–ª–µ–Ω\t\tO\n",
      ",\t\tO\n",
      "–≠—Ä–∏–∫\t\tO\n",
      "–ô–æ—Ä–Ω\t\tO\n",
      "–°—É–Ω–¥–∫–≤–∏—Å—Ç\t\tO\n",
      "#\t\tO\n",
      "–ë–æ–µ—Ü–±–µ–∑–ø—Ä–∞–≤–∏–ª\t\tO\n",
      "#\t\tO\n",
      "—Ñ–∏–ª—å–º\t\tO\n",
      "2021\t\tO\n",
      "#\t\tO\n",
      "—Ä—É—Å—Å–∫–∏–π—Ç—Ä–µ–π–ª–µ—Ä\t\tO\n",
      "#\t\tO\n",
      "–±–æ–∏–ú–ú–ê\t\tO\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([f\"{text}\\t\\t{ent}\" for (text, ent) in \n",
    "                 zip([tok.text for tok in list(tokenize(test[\"video_info\"][0]))], \n",
    "                     test.loc[0][\"entities_prediction\"])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [O, O, O, O, O, O, O, B-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞, B-–Ω–∞...\n",
       "1       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "2       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "3       [O, O, O, O, O, O, O, B-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞, B-–Ω–∞...\n",
       "4       [O, O, O, O, O, O, O, B-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è, B-–æ—Ä–≥–∞–Ω–∏–∑...\n",
       "                              ...                        \n",
       "1601    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "1602    [O, O, O, O, O, O, O, B-–ø–µ—Ä—Å–æ–Ω–∞, B-–ø–µ—Ä—Å–æ–Ω–∞, I-...\n",
       "1603    [O, O, O, O, O, O, O, O, O, O, O, B-–ø–µ—Ä—Å–æ–Ω–∞, I...\n",
       "1604    [O, O, O, O, O, O, O, B-–ø–µ—Ä—Å–æ–Ω–∞, I-–ø–µ—Ä—Å–æ–Ω–∞, I-...\n",
       "1605    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "Name: entities_prediction, Length: 1606, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"entities_prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157, 165, 150, 64, 110, 116, 131, 69, 135, 125, 62, 116, 82, 82, 95, 192, 138, 126, 71, 86, 44, 57, 91, 83, 92, 153, 92, 122, 89, 170, 84, 61, 65, 113, 125, 96, 115, 82, 70, 120, 182, 174, 139, 124, 183, 65, 151, 79, 155, 178, 117, 147, 99, 56, 154, 68, 179, 81, 46, 79, 69, 93, 110, 56, 140, 123, 117, 136, 132, 83, 126, 151, 95, 75, 68, 74, 152, 141, 125, 61, 63, 110, 107, 48, 120, 82, 84, 84, 65, 65, 142, 85, 120, 206, 87, 53, 124, 181, 173, 50, 162, 91, 162, 83, 93, 169, 53, 245, 213, 100, 81, 110, 151, 76, 132, 97, 64, 159, 108, 77, 78, 164, 94, 110, 148, 121, 103, 65, 54, 116, 190, 152, 182, 182, 87, 152, 80, 103, 104, 49, 44, 128, 79, 80, 93, 170, 127, 77, 99, 115, 115, 111, 165, 146, 79, 70, 51, 74, 187, 71, 88, 143, 65, 78, 93, 67, 172, 94, 135, 97, 92, 147, 70, 114, 118, 51, 164, 155, 65, 63, 77, 116, 66, 69, 67, 157, 75, 121, 92, 141, 185, 183, 104, 89, 111, 111, 61, 63, 151, 75, 106, 111, 90, 86, 68, 173, 88, 175, 188, 176, 62, 93, 59, 113, 80, 55, 94, 71, 79, 130, 181, 110, 91, 97, 74, 110, 185, 106, 110, 92, 79, 153, 132, 101, 87, 90, 179, 136, 61, 90, 154, 105, 54, 120, 74, 70, 153, 107, 66, 55, 166, 90, 115, 100, 96, 60, 139, 63, 94, 79, 85, 46, 81, 67, 185, 103, 80, 114, 53, 69, 175, 124, 138, 125, 67, 137, 99, 140, 58, 76, 87, 103, 57, 57, 81, 148, 87, 111, 44, 97, 92, 94, 58, 70, 144, 86, 68, 143, 93, 89, 72, 105, 201, 91, 111, 179, 88, 115, 160, 66, 89, 68, 109, 155, 63, 74, 93, 122, 131, 112, 171, 109, 100, 125, 177, 73, 123, 143, 59, 71, 164, 133, 105, 88, 94, 167, 106, 124, 163, 45, 213, 112, 74, 79, 138, 64, 181, 54, 70, 130, 175, 156, 74, 59, 68, 88, 78, 74, 179, 92, 59, 50, 109, 99, 88, 57, 191, 60, 91, 241, 51, 111, 52, 181, 113, 51, 129, 79, 95, 151, 65, 79, 115, 103, 65, 93, 88, 62, 74, 56, 106, 89, 58, 155, 107, 109, 65, 68, 197, 48, 63, 105, 108, 89, 135, 123, 143, 96, 153, 106, 63, 127, 110, 153, 134, 187, 108, 74, 82, 119, 69, 82, 148, 103, 80, 102, 104, 82, 118, 163, 139, 67, 82, 72, 122, 74, 145, 75, 63, 57, 204, 67, 183, 126, 68, 84, 125, 82, 97, 84, 89, 69, 98, 175, 52, 109, 142, 152, 157, 126, 138, 170, 57, 186, 54, 73, 103, 64, 149, 48, 210, 63, 86, 175, 90, 98, 71, 65, 66, 92, 100, 117, 182, 221, 101, 84, 116, 69, 105, 176, 70, 166, 204, 64, 89, 151, 113, 189, 110, 65, 60, 96, 72, 196, 185, 59, 97, 176, 62, 61, 58, 133, 164, 106, 167, 91, 66, 94, 52, 191, 125, 104, 179, 68, 98, 118, 115, 87, 161, 65, 56, 92, 60, 68, 135, 70, 77, 105, 49, 178, 122, 114, 69, 150, 132, 161, 73, 125, 98, 87, 61, 161, 82, 103, 54, 87, 87, 128, 73, 73, 113, 138, 111, 150, 78, 87, 71, 164, 91, 105, 113, 53, 112, 73, 88, 131, 152, 127, 57, 91, 126, 72, 65, 44, 49, 100, 121, 87, 51, 84, 103, 71, 117, 113, 126, 119, 78, 89, 71, 89, 113, 138, 99, 171, 96, 65, 62, 197, 139, 87, 55, 129, 70, 56, 60, 70, 63, 436, 88, 85, 194, 79, 170, 131, 126, 196, 65, 98, 72, 151, 65, 77, 106, 78, 90, 104, 149, 67, 167, 65, 49, 127, 67, 185, 67, 78, 55, 71, 200, 137, 104, 72, 74, 109, 91, 57, 101, 89, 119, 75, 93, 72, 181, 59, 139, 138, 61, 97, 65, 74, 169, 97, 188, 98, 227, 84, 136, 112, 77, 87, 92, 88, 84, 97, 146, 106, 84, 64, 147, 70, 64, 61, 100, 107, 184, 173, 88, 79, 110, 116, 52, 108, 171, 130, 99, 191, 80, 145, 165, 112, 101, 143, 101, 168, 51, 80, 145, 110, 74, 148, 174, 147, 84, 108, 141, 78, 157, 59, 121, 63, 103, 124, 83, 135, 136, 79, 123, 69, 155, 71, 135, 186, 189, 64, 82, 127, 74, 85, 140, 138, 101, 152, 60, 57, 72, 173, 94, 161, 162, 149, 144, 162, 150, 161, 79, 73, 188, 108, 63, 58, 70, 57, 102, 122, 91, 112, 51, 197, 79, 123, 118, 115, 96, 175, 223, 115, 48, 74, 85, 44, 67, 64, 43, 111, 106, 156, 99, 202, 154, 160, 83, 139, 101, 118, 208, 71, 103, 126, 77, 68, 184, 69, 115, 56, 132, 132, 68, 196, 98, 154, 71, 60, 187, 65, 72, 62, 82, 102, 107, 79, 118, 132, 136, 176, 155, 172, 60, 87, 104, 80, 90, 80, 118, 103, 188, 90, 51, 72, 80, 152, 72, 147, 75, 132, 126, 121, 76, 138, 115, 239, 148, 67, 122, 151, 80, 146, 115, 81, 123, 48, 156, 84, 144, 142, 116, 64, 114, 144, 166, 53, 83, 115, 99, 176, 157, 178, 76, 87, 88, 88, 82, 64, 92, 56, 54, 55, 146, 124, 70, 51, 113, 122, 73, 238, 269, 68, 155, 79, 113, 138, 160, 208, 105, 106, 129, 170, 108, 62, 91, 156, 77, 89, 165, 111, 203, 125, 85, 98, 73, 79, 154, 77, 67, 178, 133, 96, 134, 68, 47, 53, 69, 143, 49, 50, 76, 144, 171, 56, 105, 75, 99, 150, 98, 121, 199, 86, 58, 152, 177, 64, 67, 174, 116, 88, 94, 173, 72, 85, 127, 67, 137, 137, 83, 138, 81, 127, 105, 104, 77, 87, 90, 67, 110, 94, 79, 213, 97, 68, 71, 139, 139, 125, 68, 115, 148, 105, 93, 109, 149, 197, 62, 57, 95, 81, 39, 139, 164, 72, 154, 73, 194, 83, 116, 115, 86, 101, 99, 123, 172, 112, 59, 139, 166, 74, 114, 58, 178, 127, 78, 81, 125, 94, 62, 128, 59, 73, 135, 56, 86, 99, 114, 64, 107, 105, 176, 68, 186, 73, 92, 51, 63, 86, 150, 96, 99, 70, 54, 95, 75, 186, 62, 118, 137, 73, 187, 60, 88, 58, 62, 77, 98, 53, 79, 80, 183, 174, 97, 268, 111, 72, 73, 190, 165, 70, 108, 162, 103, 91, 184, 62, 168, 98, 115, 126, 72, 171, 81, 80, 85, 68, 75, 111, 128, 100, 105, 124, 193, 111, 92, 51, 98, 116, 67, 74, 142, 59, 153, 90, 118, 61, 177, 79, 104, 61, 61, 80, 92, 72, 232, 92, 195, 92, 100, 70, 116, 66, 134, 84, 66, 78, 51, 54, 168, 67, 70, 194, 149, 85, 119, 118, 84, 98, 110, 70, 81, 101, 96, 165, 93, 79, 177, 179, 144, 77, 117, 120, 117, 88, 115, 119, 80, 163, 184, 52, 122, 207, 47, 84, 73, 144, 57, 174, 93, 115, 53, 49, 91, 92, 55, 168, 80, 134, 93, 131, 154, 105, 110, 47, 91, 181, 51, 150, 142, 111, 93, 125, 74, 132, 121, 70, 165, 71, 177, 119, 77, 158, 63, 154, 51, 91, 167, 63, 158, 158, 64, 57, 171, 67, 129, 65, 103, 63, 167, 65, 193, 155, 141, 69, 68, 96, 76, 71, 78, 82, 115, 57, 92, 52, 185, 180, 66, 81, 67, 73, 102, 149, 61, 174, 181, 151, 98, 70, 98, 58, 119, 124, 98, 131, 76, 115, 28, 71, 146, 183, 76, 185, 69, 110, 142, 108, 107, 85, 185, 63, 162, 95, 144, 70, 112, 175, 168, 57, 51, 175, 182, 205, 49, 84, 178, 66, 73, 64, 77, 115, 200, 76, 103, 158, 61, 144, 85, 69, 150, 81, 69, 161, 107, 113, 175, 136, 77, 55, 125, 84, 178, 100, 165, 69, 135, 121, 71, 86, 81, 124, 81, 120, 59, 56, 61, 134, 142, 110, 178, 58, 177, 139, 89, 61, 184, 108, 58, 82, 55, 168, 113, 102, 120, 87, 137, 146, 111, 142, 135, 148, 57, 173, 128, 195, 136, 124, 201, 157, 78, 68, 86, 168, 77, 119, 63, 151, 49, 69, 71, 139, 79, 161, 67, 163, 68, 123, 55, 150, 124, 92, 190, 93, 137, 195, 94, 57, 172, 81, 116, 72, 65, 165, 95, 82, 65, 49, 56, 100, 79, 104, 84, 136, 70, 171, 92, 88, 73, 101, 149, 131, 54, 65, 138, 136, 135, 68, 51, 136, 66, 66, 116, 56, 96, 65, 91, 177, 88, 191, 76, 85, 126, 63, 85, 82, 60, 161, 85, 74, 65, 161, 173, 170, 62, 207, 108, 84, 95, 132, 62, 94, 171, 164, 66, 210, 91, 182, 177, 123, 152, 99, 77, 100, 109, 137, 67, 173, 65, 158, 116, 64, 193, 57, 49, 87, 135, 61, 167, 124, 46, 53, 66, 56, 123, 171, 108, 80, 57, 81, 148, 110, 128, 76, 135, 100, 80, 81, 61, 84, 100, 56, 40, 83, 110, 103, 87, 71, 119, 135, 114, 132, 151, 111, 128, 89, 190, 137, 139, 73, 128, 180, 131, 270, 141, 166, 60, 69, 53, 147, 90, 144, 156, 108, 150, 43, 181, 168, 192, 65, 128, 154, 112, 134, 211, 95, 101, 143, 112, 177, 84, 68, 158, 67, 60, 108, 84, 64, 160, 109, 103, 84, 128, 126, 114, 158, 67, 66, 132, 52, 96, 109, 98, 184, 119, 91, 151, 66, 182, 77, 90, 90, 62, 64, 97, 135, 143, 80, 66, 143, 57, 91, 55, 112]\n"
     ]
    }
   ],
   "source": [
    "print(test[\"entities_prediction\"].apply(len).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ë–û–ï–¶ –ë–ï–ó –ü–†–ê–í–ò–õ, –¢–†–ï–ô–õ–ï–† –Ω–∞ —Ä—É—Å—Å–∫–æ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞, B-–Ω–∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –®–û–ö! –ö–ê–ö –ñ–ï –≠–¢–û–ú–£ –ë–û–ú–ñ–£ –í–ï–ó–ï–¢ –í Br...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –î–†–ï–í–ù–ò–ï –†–û–ë–û–¢–´ NATASHA TALON –ò LEO...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ñ–ï–ù–ê –ü–£–¢–ï–®–ï–°–¢–í–ï–ù–ù–ò–ö–ê –í–û –í–†–ï–ú–ï–ù–ò = ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞, B-–Ω–∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ö–∏–Ω–æ–∑–∞–ª –î–ö –ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç –Ω–∞ –º—É–ª—å—Ç—Ñ–∏–ª—å...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è, B-–æ—Ä–≥–∞–Ω–∏–∑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ú–∏–ª–µ–π—à–∏–µ –¥–µ—Ç–µ–Ω—ã—à–∏ –∂–∏–≤–æ—Ç–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –î–∏–∞–Ω–∞ –®—É—Ä—ã–≥–∏–Ω–∞ –Ω–∞–±—Ä–æ—Å–∏–ª–∞—Å—å –Ω–∞ –∑—Ä–∏—Ç...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-–ø–µ—Ä—Å–æ–Ω–∞, B-–ø–µ—Ä—Å–æ–Ω–∞, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ì—É–±–µ—Ä–Ω–∞—Ç–æ—Ä –ï–≤–≥–µ–Ω–∏–π –ö—É–π–≤–∞—à–µ–≤ –∏ –Ω–µ–ø–æ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-–ø–µ—Ä—Å–æ–Ω–∞, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ì–æ—Ä–µ–ª–æ–≤ = –æ –≤–∞–∫—Ü–∏–Ω–∞—Ü–∏–∏ –æ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-–ø–µ—Ä—Å–æ–Ω–∞, I-–ø–µ—Ä—Å–æ–Ω–∞, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –°–º–æ—Ç—Ä–µ–ª–∞ –£–ñ–ê–°–¢–ò–ö –∏ –∑–∞–±—ã–ª–∞, —á—Ç–æ –Ω–µ ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1606 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             video_info  \\\n",
       "0     <–ù–ê–ó–í–ê–ù–ò–ï:> –ë–û–ï–¶ –ë–ï–ó –ü–†–ê–í–ò–õ, –¢–†–ï–ô–õ–ï–† –Ω–∞ —Ä—É—Å—Å–∫–æ...   \n",
       "1     <–ù–ê–ó–í–ê–ù–ò–ï:> –®–û–ö! –ö–ê–ö –ñ–ï –≠–¢–û–ú–£ –ë–û–ú–ñ–£ –í–ï–ó–ï–¢ –í Br...   \n",
       "2     <–ù–ê–ó–í–ê–ù–ò–ï:> –î–†–ï–í–ù–ò–ï –†–û–ë–û–¢–´ NATASHA TALON –ò LEO...   \n",
       "3     <–ù–ê–ó–í–ê–ù–ò–ï:> –ñ–ï–ù–ê –ü–£–¢–ï–®–ï–°–¢–í–ï–ù–ù–ò–ö–ê –í–û –í–†–ï–ú–ï–ù–ò = ...   \n",
       "4     <–ù–ê–ó–í–ê–ù–ò–ï:> –ö–∏–Ω–æ–∑–∞–ª –î–ö –ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç –Ω–∞ –º—É–ª—å—Ç—Ñ–∏–ª—å...   \n",
       "...                                                 ...   \n",
       "1601  <–ù–ê–ó–í–ê–ù–ò–ï:> –ú–∏–ª–µ–π—à–∏–µ –¥–µ—Ç–µ–Ω—ã—à–∏ –∂–∏–≤–æ—Ç–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã...   \n",
       "1602  <–ù–ê–ó–í–ê–ù–ò–ï:> –î–∏–∞–Ω–∞ –®—É—Ä—ã–≥–∏–Ω–∞ –Ω–∞–±—Ä–æ—Å–∏–ª–∞—Å—å –Ω–∞ –∑—Ä–∏—Ç...   \n",
       "1603  <–ù–ê–ó–í–ê–ù–ò–ï:> –ì—É–±–µ—Ä–Ω–∞—Ç–æ—Ä –ï–≤–≥–µ–Ω–∏–π –ö—É–π–≤–∞—à–µ–≤ –∏ –Ω–µ–ø–æ...   \n",
       "1604  <–ù–ê–ó–í–ê–ù–ò–ï:> –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ì–æ—Ä–µ–ª–æ–≤ = –æ –≤–∞–∫—Ü–∏–Ω–∞—Ü–∏–∏ –æ...   \n",
       "1605  <–ù–ê–ó–í–ê–ù–ò–ï:> –°–º–æ—Ç—Ä–µ–ª–∞ –£–ñ–ê–°–¢–ò–ö –∏ –∑–∞–±—ã–ª–∞, —á—Ç–æ –Ω–µ ...   \n",
       "\n",
       "                                    entities_prediction  \n",
       "0     [O, O, O, O, O, O, O, B-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞, B-–Ω–∞...  \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3     [O, O, O, O, O, O, O, B-–Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞, B-–Ω–∞...  \n",
       "4     [O, O, O, O, O, O, O, B-–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è, B-–æ—Ä–≥–∞–Ω–∏–∑...  \n",
       "...                                                 ...  \n",
       "1601  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1602  [O, O, O, O, O, O, O, B-–ø–µ—Ä—Å–æ–Ω–∞, B-–ø–µ—Ä—Å–æ–Ω–∞, I-...  \n",
       "1603  [O, O, O, O, O, O, O, O, O, O, O, B-–ø–µ—Ä—Å–æ–Ω–∞, I...  \n",
       "1604  [O, O, O, O, O, O, O, B-–ø–µ—Ä—Å–æ–Ω–∞, I-–ø–µ—Ä—Å–æ–Ω–∞, I-...  \n",
       "1605  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "\n",
       "[1606 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"submit_xlm_roberta_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ë–û–ï–¶ –ë–ï–ó –ü–†–ê–í–ò–õ, –¢–†–ï–ô–õ–ï–† –Ω–∞ —Ä—É—Å—Å–∫–æ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-–Ω–∞–∑–≤–∞–Ω–∏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –®–û–ö! –ö–ê–ö –ñ–ï –≠–¢–û–ú–£ –ë–û–ú–ñ–£ –í–ï–ó–ï–¢ –í Br...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –î–†–ï–í–ù–ò–ï –†–û–ë–û–¢–´ NATASHA TALON –ò LEO...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ñ–ï–ù–ê –ü–£–¢–ï–®–ï–°–¢–í–ï–ù–ù–ò–ö–ê –í–û –í–†–ï–ú–ï–ù–ò = ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-–Ω–∞–∑–≤–∞–Ω–∏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ö–∏–Ω–æ–∑–∞–ª –î–ö –ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç –Ω–∞ –º—É–ª—å—Ç—Ñ–∏–ª—å...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-–æ—Ä–≥–∞–Ω–∏–∑...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ú–∏–ª–µ–π—à–∏–µ –¥–µ—Ç–µ–Ω—ã—à–∏ –∂–∏–≤–æ—Ç–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –î–∏–∞–Ω–∞ –®—É—Ä—ã–≥–∏–Ω–∞ –Ω–∞–±—Ä–æ—Å–∏–ª–∞—Å—å –Ω–∞ –∑—Ä–∏—Ç...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-–ø–µ—Ä—Å–æ–Ω–∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ì—É–±–µ—Ä–Ω–∞—Ç–æ—Ä –ï–≤–≥–µ–Ω–∏–π –ö—É–π–≤–∞—à–µ–≤ –∏ –Ω–µ–ø–æ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ì–æ—Ä–µ–ª–æ–≤ = –æ –≤–∞–∫—Ü–∏–Ω–∞—Ü–∏–∏ –æ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-–ø–µ—Ä—Å–æ–Ω–∞...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –°–º–æ—Ç—Ä–µ–ª–∞ –£–ñ–ê–°–¢–ò–ö –∏ –∑–∞–±—ã–ª–∞, —á—Ç–æ –Ω–µ ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1606 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             video_info  \\\n",
       "0     <–ù–ê–ó–í–ê–ù–ò–ï:> –ë–û–ï–¶ –ë–ï–ó –ü–†–ê–í–ò–õ, –¢–†–ï–ô–õ–ï–† –Ω–∞ —Ä—É—Å—Å–∫–æ...   \n",
       "1     <–ù–ê–ó–í–ê–ù–ò–ï:> –®–û–ö! –ö–ê–ö –ñ–ï –≠–¢–û–ú–£ –ë–û–ú–ñ–£ –í–ï–ó–ï–¢ –í Br...   \n",
       "2     <–ù–ê–ó–í–ê–ù–ò–ï:> –î–†–ï–í–ù–ò–ï –†–û–ë–û–¢–´ NATASHA TALON –ò LEO...   \n",
       "3     <–ù–ê–ó–í–ê–ù–ò–ï:> –ñ–ï–ù–ê –ü–£–¢–ï–®–ï–°–¢–í–ï–ù–ù–ò–ö–ê –í–û –í–†–ï–ú–ï–ù–ò = ...   \n",
       "4     <–ù–ê–ó–í–ê–ù–ò–ï:> –ö–∏–Ω–æ–∑–∞–ª –î–ö –ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç –Ω–∞ –º—É–ª—å—Ç—Ñ–∏–ª—å...   \n",
       "...                                                 ...   \n",
       "1601  <–ù–ê–ó–í–ê–ù–ò–ï:> –ú–∏–ª–µ–π—à–∏–µ –¥–µ—Ç–µ–Ω—ã—à–∏ –∂–∏–≤–æ—Ç–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã...   \n",
       "1602  <–ù–ê–ó–í–ê–ù–ò–ï:> –î–∏–∞–Ω–∞ –®—É—Ä—ã–≥–∏–Ω–∞ –Ω–∞–±—Ä–æ—Å–∏–ª–∞—Å—å –Ω–∞ –∑—Ä–∏—Ç...   \n",
       "1603  <–ù–ê–ó–í–ê–ù–ò–ï:> –ì—É–±–µ—Ä–Ω–∞—Ç–æ—Ä –ï–≤–≥–µ–Ω–∏–π –ö—É–π–≤–∞—à–µ–≤ –∏ –Ω–µ–ø–æ...   \n",
       "1604  <–ù–ê–ó–í–ê–ù–ò–ï:> –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ì–æ—Ä–µ–ª–æ–≤ = –æ –≤–∞–∫—Ü–∏–Ω–∞—Ü–∏–∏ –æ...   \n",
       "1605  <–ù–ê–ó–í–ê–ù–ò–ï:> –°–º–æ—Ç—Ä–µ–ª–∞ –£–ñ–ê–°–¢–ò–ö –∏ –∑–∞–±—ã–ª–∞, —á—Ç–æ –Ω–µ ...   \n",
       "\n",
       "                                    entities_prediction  \n",
       "0     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-–Ω–∞–∑–≤–∞–Ω–∏...  \n",
       "1     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "2     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "3     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-–Ω–∞–∑–≤–∞–Ω–∏...  \n",
       "4     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-–æ—Ä–≥–∞–Ω–∏–∑...  \n",
       "...                                                 ...  \n",
       "1601  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1602  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-–ø–µ—Ä—Å–æ–Ω–∞...  \n",
       "1603  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1604  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-–ø–µ—Ä—Å–æ–Ω–∞...  \n",
       "1605  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "\n",
       "[1606 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"submit_xlm_roberta_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ë–û–ï–¶ –ë–ï–ó –ü–†–ê–í–ò–õ, –¢–†–ï–ô–õ–ï–† –Ω–∞ —Ä—É—Å—Å–∫–æ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –®–û–ö! –ö–ê–ö –ñ–ï –≠–¢–û–ú–£ –ë–û–ú–ñ–£ –í–ï–ó–ï–¢ –í Br...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –î–†–ï–í–ù–ò–ï –†–û–ë–û–¢–´ NATASHA TALON –ò LEO...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ñ–ï–ù–ê –ü–£–¢–ï–®–ï–°–¢–í–ï–ù–ù–ò–ö–ê –í–û –í–†–ï–ú–ï–ù–ò = ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ö–∏–Ω–æ–∑–∞–ª –î–ö –ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç –Ω–∞ –º—É–ª—å—Ç—Ñ–∏–ª—å...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ú–∏–ª–µ–π—à–∏–µ –¥–µ—Ç–µ–Ω—ã—à–∏ –∂–∏–≤–æ—Ç–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –î–∏–∞–Ω–∞ –®—É—Ä—ã–≥–∏–Ω–∞ –Ω–∞–±—Ä–æ—Å–∏–ª–∞—Å—å –Ω–∞ –∑—Ä–∏—Ç...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ì—É–±–µ—Ä–Ω–∞—Ç–æ—Ä –ï–≤–≥–µ–Ω–∏–π –ö—É–π–≤–∞—à–µ–≤ –∏ –Ω–µ–ø–æ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ì–æ—Ä–µ–ª–æ–≤ = –æ –≤–∞–∫—Ü–∏–Ω–∞—Ü–∏–∏ –æ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>&lt;–ù–ê–ó–í–ê–ù–ò–ï:&gt; –°–º–æ—Ç—Ä–µ–ª–∞ –£–ñ–ê–°–¢–ò–ö –∏ –∑–∞–±—ã–ª–∞, —á—Ç–æ –Ω–µ ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1606 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             video_info  \\\n",
       "0     <–ù–ê–ó–í–ê–ù–ò–ï:> –ë–û–ï–¶ –ë–ï–ó –ü–†–ê–í–ò–õ, –¢–†–ï–ô–õ–ï–† –Ω–∞ —Ä—É—Å—Å–∫–æ...   \n",
       "1     <–ù–ê–ó–í–ê–ù–ò–ï:> –®–û–ö! –ö–ê–ö –ñ–ï –≠–¢–û–ú–£ –ë–û–ú–ñ–£ –í–ï–ó–ï–¢ –í Br...   \n",
       "2     <–ù–ê–ó–í–ê–ù–ò–ï:> –î–†–ï–í–ù–ò–ï –†–û–ë–û–¢–´ NATASHA TALON –ò LEO...   \n",
       "3     <–ù–ê–ó–í–ê–ù–ò–ï:> –ñ–ï–ù–ê –ü–£–¢–ï–®–ï–°–¢–í–ï–ù–ù–ò–ö–ê –í–û –í–†–ï–ú–ï–ù–ò = ...   \n",
       "4     <–ù–ê–ó–í–ê–ù–ò–ï:> –ö–∏–Ω–æ–∑–∞–ª –î–ö –ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç –Ω–∞ –º—É–ª—å—Ç—Ñ–∏–ª—å...   \n",
       "...                                                 ...   \n",
       "1601  <–ù–ê–ó–í–ê–ù–ò–ï:> –ú–∏–ª–µ–π—à–∏–µ –¥–µ—Ç–µ–Ω—ã—à–∏ –∂–∏–≤–æ—Ç–Ω—ã—Ö, –∫–æ—Ç–æ—Ä—ã...   \n",
       "1602  <–ù–ê–ó–í–ê–ù–ò–ï:> –î–∏–∞–Ω–∞ –®—É—Ä—ã–≥–∏–Ω–∞ –Ω–∞–±—Ä–æ—Å–∏–ª–∞—Å—å –Ω–∞ –∑—Ä–∏—Ç...   \n",
       "1603  <–ù–ê–ó–í–ê–ù–ò–ï:> –ì—É–±–µ—Ä–Ω–∞—Ç–æ—Ä –ï–≤–≥–µ–Ω–∏–π –ö—É–π–≤–∞—à–µ–≤ –∏ –Ω–µ–ø–æ...   \n",
       "1604  <–ù–ê–ó–í–ê–ù–ò–ï:> –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ì–æ—Ä–µ–ª–æ–≤ = –æ –≤–∞–∫—Ü–∏–Ω–∞—Ü–∏–∏ –æ...   \n",
       "1605  <–ù–ê–ó–í–ê–ù–ò–ï:> –°–º–æ—Ç—Ä–µ–ª–∞ –£–ñ–ê–°–¢–ò–ö –∏ –∑–∞–±—ã–ª–∞, —á—Ç–æ –Ω–µ ...   \n",
       "\n",
       "                                    entities_prediction  \n",
       "0     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "2     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "3     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "4     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "...                                                 ...  \n",
       "1601  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1602  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1603  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1604  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1605  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "\n",
       "[1606 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"sample_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
