{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Персона — челоке/персонаж, упоминаемый в тексте (Например: Настя Ивлеева, Дядя Федор, Баста, Евгений Онегин). Персона - только имя человека/персонажа. Например, фраза \"Руководитель отдела\" не является персоной.\n",
    "\n",
    "2. Локация — географические координаты, такие как страна, город, край, область, название пространства (Например: Москва, Россия, Казахстан, на Фестивальной улице, гостица Солнечная)\n",
    "\n",
    "3. Дата — дата события (Например: 4 января, в 1992 году, в октябре)\n",
    "\n",
    "4. Организация — организация, орган государственной власти, учреждение (Например: Яндекс, Министерство цифрового развития, Совет Федерации, ООО \"Рога и Копыта\")\n",
    "\n",
    "5. Бренд - бренд (Например:  Samsung, Audi, Toyota, )\n",
    "\n",
    "6. Модель - название модели (Например: Galaxy S10, RS6, Corolla)\n",
    "\n",
    "7. Название проекта - название шоу, сериала, фильма, проекта организации (Например: Битва экстрасенсов, беременна в 16, Интерны, Лидеры России, Международная кооперация и экспорт)\n",
    "\n",
    "8. Сезон - сезон/часть сериала (Например: третий, первый, 1)\n",
    "\n",
    "9. Серия - серия сериала, передачи (Например: первая, заключительная, 4)\n",
    "\n",
    "10. Лига - название спортивной лиги (Например: Чемпионат Европы-2024, Лалига, Английская премьер лига, Восточно-европейская хоккейная лига, чемпионате мира)\n",
    "\n",
    "11. Команда - команда (спортивная, студенческая) (Например: ЦСКА, Manchester United, КАМАЗ-мастер)\n",
    "\n",
    "12. Вид спорта - название спорта или подтипа (Например: футбол, керлинг, автоспорт, танго)\n",
    "\n",
    "13. Видеоигра — название видеоигры (Например: GTA, Call of Duty, Майнкрафт)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание среды проекта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers evaluate  install accelerate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76521/1682736822.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import razdel\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Агент 117: Из Африки с любовью — Р...</td>\n",
       "      <td>{\"label\":\"локация\"\\,\"offset\":26\\,\"length\":6\\,\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Коленвал Инфинити Ку икс 56= 5.6 V...</td>\n",
       "      <td>{\"label\":\"организация\"\\,\"offset\":196\\,\"length\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ВЫЗОВ ДЕМОНА = Вызвал Серого Челов...</td>\n",
       "      <td>{\"label\":\"название проекта\"\\,\"offset\":12\\,\"len...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Довоенная немецкая кирха в Калинин...</td>\n",
       "      <td>{\"label\":\"не найдено\"\\,\"offset\":162\\,\"length\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; \"Спартаку\" помогли судьи? Локомоти...</td>\n",
       "      <td>{\"label\":\"команда\"\\,\"offset\":13\\,\"length\":8\\,\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_info  \\\n",
       "0  <НАЗВАНИЕ:> Агент 117: Из Африки с любовью — Р...   \n",
       "1  <НАЗВАНИЕ:> Коленвал Инфинити Ку икс 56= 5.6 V...   \n",
       "2  <НАЗВАНИЕ:> ВЫЗОВ ДЕМОНА = Вызвал Серого Челов...   \n",
       "3  <НАЗВАНИЕ:> Довоенная немецкая кирха в Калинин...   \n",
       "4  <НАЗВАНИЕ:> \"Спартаку\" помогли судьи? Локомоти...   \n",
       "\n",
       "                                            entities  \n",
       "0  {\"label\":\"локация\"\\,\"offset\":26\\,\"length\":6\\,\"...  \n",
       "1  {\"label\":\"организация\"\\,\"offset\":196\\,\"length\"...  \n",
       "2  {\"label\":\"название проекта\"\\,\"offset\":12\\,\"len...  \n",
       "3  {\"label\":\"не найдено\"\\,\"offset\":162\\,\"length\":...  \n",
       "4  {\"label\":\"команда\"\\,\"offset\":13\\,\"length\":8\\,\"...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"ner_data_train.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Агент 117: Из Африки с любовью — Р...</td>\n",
       "      <td>[{'label': 'локация', 'offset': 26, 'length': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Коленвал Инфинити Ку икс 56= 5.6 V...</td>\n",
       "      <td>[{'label': 'организация', 'offset': 196, 'leng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ВЫЗОВ ДЕМОНА = Вызвал Серого Челов...</td>\n",
       "      <td>[{'label': 'название проекта', 'offset': 12, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Довоенная немецкая кирха в Калинин...</td>\n",
       "      <td>[{'label': 'не найдено', 'offset': 162, 'lengt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; \"Спартаку\" помогли судьи? Локомоти...</td>\n",
       "      <td>[{'label': 'команда', 'offset': 13, 'length': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_info  \\\n",
       "0  <НАЗВАНИЕ:> Агент 117: Из Африки с любовью — Р...   \n",
       "1  <НАЗВАНИЕ:> Коленвал Инфинити Ку икс 56= 5.6 V...   \n",
       "2  <НАЗВАНИЕ:> ВЫЗОВ ДЕМОНА = Вызвал Серого Челов...   \n",
       "3  <НАЗВАНИЕ:> Довоенная немецкая кирха в Калинин...   \n",
       "4  <НАЗВАНИЕ:> \"Спартаку\" помогли судьи? Локомоти...   \n",
       "\n",
       "                                            entities  \n",
       "0  [{'label': 'локация', 'offset': 26, 'length': ...  \n",
       "1  [{'label': 'организация', 'offset': 196, 'leng...  \n",
       "2  [{'label': 'название проекта', 'offset': 12, '...  \n",
       "3  [{'label': 'не найдено', 'offset': 162, 'lengt...  \n",
       "4  [{'label': 'команда', 'offset': 13, 'length': ...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# данные спарсены с Толоки, поэтому могут иметь проблемы с символами и их нужно избежать, \n",
    "# удалить лишние '\\' например, преобразовать из str в список dict-ов\n",
    "df = data.copy()\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\,', ',')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\\\\\\\', '\\\\')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: '[' + l + ']'if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: json.loads(l)if isinstance(l, str) else l)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь из наших данных нам нужно извлечь для каждого слова (токена) его\n",
    "# тег (label) из разметки, чтобы потом предать в модель классификации токенов\n",
    "\n",
    "def extract_labels(item):\n",
    "    \n",
    "    # воспользуемся удобным токенайзером из библиотеки razdel, \n",
    "    # она помимо разбиения на слова, сохраняет важные\n",
    "    # для нас числа - начало и конец слова в токенах\n",
    "    \n",
    "    raw_toks = list(razdel.tokenize(item['video_info']))\n",
    "    words = [tok.text for tok in raw_toks]\n",
    "    # присвоим для начала каждому слову тег 'О' - тег, означающий отсутствие NER-а\n",
    "    word_labels = ['O'] * len(raw_toks)\n",
    "    char2word = [None] * len(item['video_info'])\n",
    "    # так как NER можем состаять из нескольких слов, то нам нужно сохранить эту инфорцию\n",
    "    for i, word in enumerate(raw_toks):\n",
    "        char2word[word.start:word.stop] = [i] * len(word.text)\n",
    "\n",
    "    labels = item['entities']\n",
    "    if isinstance(labels, dict):\n",
    "        labels = [labels]\n",
    "    if labels is not None:\n",
    "        for e in labels:\n",
    "            if e['label'] != 'не найдено':\n",
    "                e_words = sorted({idx for idx in char2word[e['offset']:e['offset']+e['length']] if idx is not None})\n",
    "                if e_words:\n",
    "                    word_labels[e_words[0]] = 'B-' + e['label']\n",
    "                    for idx in e_words[1:]:\n",
    "                        word_labels[idx] = 'I-' + e['label']\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        return {'tokens': words, 'tags': word_labels}\n",
    "    else: return {'tokens': words, 'tags': word_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_labels(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making train ant test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data = [extract_labels(item) for i, item in df.iterrows()]\n",
    "ner_train, ner_test = train_test_split(ner_data, test_size=0.01, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-Дата',\n",
       " 'B-бренд',\n",
       " 'B-вид спорта',\n",
       " 'B-видеоигра',\n",
       " 'B-команда',\n",
       " 'B-лига',\n",
       " 'B-локация',\n",
       " 'B-модель',\n",
       " 'B-название проекта',\n",
       " 'B-организация',\n",
       " 'B-персона',\n",
       " 'B-сезон',\n",
       " 'B-серия',\n",
       " 'I-Дата',\n",
       " 'I-бренд',\n",
       " 'I-вид спорта',\n",
       " 'I-видеоигра',\n",
       " 'I-команда',\n",
       " 'I-лига',\n",
       " 'I-локация',\n",
       " 'I-модель',\n",
       " 'I-название проекта',\n",
       " 'I-организация',\n",
       " 'I-персона',\n",
       " 'I-сезон',\n",
       " 'I-серия']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = sorted({label for item in ner_train for label in item['tags']})\n",
    "if 'O' in label_list:\n",
    "    label_list.remove('O')\n",
    "    label_list = ['O'] + label_list\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 6357\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 65\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
    "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
    "})\n",
    "ner_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"xlm-roberta-large-finetuned-conll03-english\"\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, device=device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### У Bert свой собсвенный токенайзер, который разбивает слова на мелкие токены, поэтому нам нужно корректно сопоставить токены и соответсвующие им неры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b771467445b749ae99ff9e8a6703c1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3496e0dc67423abdc9b39a008d59e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохраняем словарик соотвествия тега и его индекса внутри модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "model.config.id2label = dict(enumerate(label_list))\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = torch.nn.Linear(1024, len(label_list), bias=True).cuda()\n",
    "model.num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В качестве метрик возьмем precision, recall, accuracy, для этого воспользуемся специализированной под Ner задачу библиотеку seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76521/152412463.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    "    output_dir=\"models/xlm_roberta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"test\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.trainer import logger as noisy_logger\n",
    "# noisy_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для дообучения берта можно эксперементировать с заморозкой/разморозкой разных слоев, здесь мы оставим все слои размороженными \n",
    "# Для быстроты обучения можно заморозить всю бертовую часть, кроме классификатора, но тогда качесвто будет похуже\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='584' max='805' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [584/805 16:45 < 06:21, 0.58 it/s, Epoch 3.62/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.312765</td>\n",
       "      <td>0.534657</td>\n",
       "      <td>0.619244</td>\n",
       "      <td>0.573850</td>\n",
       "      <td>0.911907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.303146</td>\n",
       "      <td>0.530329</td>\n",
       "      <td>0.597361</td>\n",
       "      <td>0.561853</td>\n",
       "      <td>0.909396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.315811</td>\n",
       "      <td>0.521158</td>\n",
       "      <td>0.639558</td>\n",
       "      <td>0.574320</td>\n",
       "      <td>0.909185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/hacks_ai/XLM_Roberta_Training.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f736176656c6f7632227d@ssh-remote%2B172.16.204.14/workspace/hacks_ai/XLM_Roberta_Training.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_that_works/lib/python3.11/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_that_works/lib/python3.11/site-packages/transformers/trainer.py:1892\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1889\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1891\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1892\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1895\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1896\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1897\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1898\u001b[0m ):\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_that_works/lib/python3.11/site-packages/transformers/trainer.py:2787\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2786\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2787\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2789\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_that_works/lib/python3.11/site-packages/accelerate/accelerator.py:1989\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1987\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1988\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1989\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_that_works/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/torch_that_works/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39m_execution_engine\u001b[39m.\u001b[39mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/xlm_roberta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.40144357085227966,\n",
       " 'eval_precision': 0.5195944744175016,\n",
       " 'eval_recall': 0.5724737082761774,\n",
       " 'eval_f1': 0.5447538538040775,\n",
       " 'eval_accuracy': 0.905239093160602}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посчитаем метрики на отложенном датасете\n",
    "\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(sum(true_labels, []), sum(true_predictions, []), labels=label_list),\n",
    "    index=label_list,\n",
    "    columns=label_list\n",
    ")\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1222036/1682736822.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import razdel\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Агент 117: Из Африки с любовью — Р...</td>\n",
       "      <td>{\"label\":\"локация\"\\,\"offset\":26\\,\"length\":6\\,\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Коленвал Инфинити Ку икс 56= 5.6 V...</td>\n",
       "      <td>{\"label\":\"организация\"\\,\"offset\":196\\,\"length\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ВЫЗОВ ДЕМОНА = Вызвал Серого Челов...</td>\n",
       "      <td>{\"label\":\"название проекта\"\\,\"offset\":12\\,\"len...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Довоенная немецкая кирха в Калинин...</td>\n",
       "      <td>{\"label\":\"не найдено\"\\,\"offset\":162\\,\"length\":...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; \"Спартаку\" помогли судьи? Локомоти...</td>\n",
       "      <td>{\"label\":\"команда\"\\,\"offset\":13\\,\"length\":8\\,\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_info  \\\n",
       "0  <НАЗВАНИЕ:> Агент 117: Из Африки с любовью — Р...   \n",
       "1  <НАЗВАНИЕ:> Коленвал Инфинити Ку икс 56= 5.6 V...   \n",
       "2  <НАЗВАНИЕ:> ВЫЗОВ ДЕМОНА = Вызвал Серого Челов...   \n",
       "3  <НАЗВАНИЕ:> Довоенная немецкая кирха в Калинин...   \n",
       "4  <НАЗВАНИЕ:> \"Спартаку\" помогли судьи? Локомоти...   \n",
       "\n",
       "                                            entities  \n",
       "0  {\"label\":\"локация\"\\,\"offset\":26\\,\"length\":6\\,\"...  \n",
       "1  {\"label\":\"организация\"\\,\"offset\":196\\,\"length\"...  \n",
       "2  {\"label\":\"название проекта\"\\,\"offset\":12\\,\"len...  \n",
       "3  {\"label\":\"не найдено\"\\,\"offset\":162\\,\"length\":...  \n",
       "4  {\"label\":\"команда\"\\,\"offset\":13\\,\"length\":8\\,\"...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"ner_data_train.csv\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Агент 117: Из Африки с любовью — Р...</td>\n",
       "      <td>[{'label': 'локация', 'offset': 26, 'length': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Коленвал Инфинити Ку икс 56= 5.6 V...</td>\n",
       "      <td>[{'label': 'организация', 'offset': 196, 'leng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ВЫЗОВ ДЕМОНА = Вызвал Серого Челов...</td>\n",
       "      <td>[{'label': 'название проекта', 'offset': 12, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Довоенная немецкая кирха в Калинин...</td>\n",
       "      <td>[{'label': 'не найдено', 'offset': 162, 'lengt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; \"Спартаку\" помогли судьи? Локомоти...</td>\n",
       "      <td>[{'label': 'команда', 'offset': 13, 'length': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_info  \\\n",
       "0  <НАЗВАНИЕ:> Агент 117: Из Африки с любовью — Р...   \n",
       "1  <НАЗВАНИЕ:> Коленвал Инфинити Ку икс 56= 5.6 V...   \n",
       "2  <НАЗВАНИЕ:> ВЫЗОВ ДЕМОНА = Вызвал Серого Челов...   \n",
       "3  <НАЗВАНИЕ:> Довоенная немецкая кирха в Калинин...   \n",
       "4  <НАЗВАНИЕ:> \"Спартаку\" помогли судьи? Локомоти...   \n",
       "\n",
       "                                            entities  \n",
       "0  [{'label': 'локация', 'offset': 26, 'length': ...  \n",
       "1  [{'label': 'организация', 'offset': 196, 'leng...  \n",
       "2  [{'label': 'название проекта', 'offset': 12, '...  \n",
       "3  [{'label': 'не найдено', 'offset': 162, 'lengt...  \n",
       "4  [{'label': 'команда', 'offset': 13, 'length': ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# данные спарсены с Толоки, поэтому могут иметь проблемы с символами и их нужно избежать, \n",
    "# удалить лишние '\\' например, преобразовать из str в список dict-ов\n",
    "df = data.copy()\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\,', ',')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: l.replace('\\\\\\\\', '\\\\')if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: '[' + l + ']'if isinstance(l, str) else l)\n",
    "df['entities'] = df['entities'].apply(lambda l: json.loads(l)if isinstance(l, str) else l)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь из наших данных нам нужно извлечь для каждого слова (токена) его\n",
    "# тег (label) из разметки, чтобы потом предать в модель классификации токенов\n",
    "\n",
    "def extract_labels(item):\n",
    "    \n",
    "    # воспользуемся удобным токенайзером из библиотеки razdel, \n",
    "    # она помимо разбиения на слова, сохраняет важные\n",
    "    # для нас числа - начало и конец слова в токенах\n",
    "    \n",
    "    raw_toks = list(razdel.tokenize(item['video_info']))\n",
    "    words = [tok.text for tok in raw_toks]\n",
    "    # присвоим для начала каждому слову тег 'О' - тег, означающий отсутствие NER-а\n",
    "    word_labels = ['O'] * len(raw_toks)\n",
    "    char2word = [None] * len(item['video_info'])\n",
    "    # так как NER можем состаять из нескольких слов, то нам нужно сохранить эту инфорцию\n",
    "    for i, word in enumerate(raw_toks):\n",
    "        char2word[word.start:word.stop] = [i] * len(word.text)\n",
    "\n",
    "    labels = item['entities']\n",
    "    if isinstance(labels, dict):\n",
    "        labels = [labels]\n",
    "    if labels is not None:\n",
    "        for e in labels:\n",
    "            if e['label'] != 'не найдено':\n",
    "                e_words = sorted({idx for idx in char2word[e['offset']:e['offset']+e['length']] if idx is not None})\n",
    "                if e_words:\n",
    "                    word_labels[e_words[0]] = 'B-' + e['label']\n",
    "                    for idx in e_words[1:]:\n",
    "                        word_labels[idx] = 'I-' + e['label']\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        return {'tokens': words, 'tags': word_labels}\n",
    "    else: return {'tokens': words, 'tags': word_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_labels(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making train ant test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data = [extract_labels(item) for i, item in df.iterrows()]\n",
    "ner_train, ner_test = train_test_split(ner_data, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-Дата',\n",
       " 'B-бренд',\n",
       " 'B-вид спорта',\n",
       " 'B-видеоигра',\n",
       " 'B-команда',\n",
       " 'B-лига',\n",
       " 'B-локация',\n",
       " 'B-модель',\n",
       " 'B-название проекта',\n",
       " 'B-организация',\n",
       " 'B-персона',\n",
       " 'B-сезон',\n",
       " 'B-серия',\n",
       " 'I-Дата',\n",
       " 'I-бренд',\n",
       " 'I-вид спорта',\n",
       " 'I-видеоигра',\n",
       " 'I-команда',\n",
       " 'I-лига',\n",
       " 'I-локация',\n",
       " 'I-модель',\n",
       " 'I-название проекта',\n",
       " 'I-организация',\n",
       " 'I-персона',\n",
       " 'I-сезон',\n",
       " 'I-серия']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = sorted({label for item in ner_train for label in item['tags']})\n",
    "if 'O' in label_list:\n",
    "    label_list.remove('O')\n",
    "    label_list = ['O'] + label_list\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 6357\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'tags'],\n",
       "        num_rows: 65\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data = DatasetDict({\n",
    "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
    "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
    "})\n",
    "ner_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint = \"xlm-roberta-large\"\n",
    "batch_size = 32\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, device=device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### У Bert свой собсвенный токенайзер, который разбивает слова на мелкие токены, поэтому нам нужно корректно сопоставить токены и соответсвующие им неры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8989f3e7be41afb3ec3aa1c9e36a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5137 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039e8623cc504bf190bf8ff5e28c6941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохраняем словарик соотвествия тега и его индекса внутри модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "model.config.id2label = dict(enumerate(label_list))\n",
    "model.config.label2id = {v: k for k, v in model.config.id2label.items()}\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = torch.nn.Linear(1024, len(label_list), bias=True).cuda()\n",
    "model.num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1024, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В качестве метрик возьмем precision, recall, accuracy, для этого воспользуемся специализированной под Ner задачу библиотеку seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=7,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    "    output_dir=\"models/xlm_roberta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"test\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.trainer import logger as noisy_logger\n",
    "# noisy_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для дообучения берта можно эксперементировать с заморозкой/разморозкой разных слоев, здесь мы оставим все слои размороженными \n",
    "# Для быстроты обучения можно заморозить всю бертовую часть, кроме классификатора, но тогда качесвто будет похуже\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='161' max='161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [161/161 04:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.332525</td>\n",
       "      <td>0.525924</td>\n",
       "      <td>0.627474</td>\n",
       "      <td>0.572229</td>\n",
       "      <td>0.909663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=161, training_loss=0.198312332911521, metrics={'train_runtime': 282.9288, 'train_samples_per_second': 18.157, 'train_steps_per_second': 0.569, 'total_flos': 3367775178141534.0, 'train_loss': 0.198312332911521, 'epoch': 1.0})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/xlm_roberta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.40144357085227966,\n",
       " 'eval_precision': 0.5195944744175016,\n",
       " 'eval_recall': 0.5724737082761774,\n",
       " 'eval_f1': 0.5447538538040775,\n",
       " 'eval_accuracy': 0.905239093160602}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посчитаем метрики на отложенном датасете\n",
    "\n",
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(sum(true_labels, []), sum(true_predictions, []), labels=label_list),\n",
    "    index=label_list,\n",
    "    columns=label_list\n",
    ")\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=model, tokenizer=tokenizer, task='ner', aggregation_strategy='average', device='cuda')\n",
    "\n",
    "def predict_ner(text, tokenizer, model, pipe, verbose=True):\n",
    "    tokens = tokenizer(text, truncation=True, is_split_into_words=True, return_tensors='pt')\n",
    "    tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred = model(**tokens)\n",
    "    # print(pred.logits.shape)\n",
    "    indices = pred.logits.argmax(dim=-1)[0].cpu().numpy()\n",
    "    token_text = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "    labels = []\n",
    "    for t, idx in zip(token_text, indices):\n",
    "        if '##' not in t:\n",
    "            labels.append(label_list[idx])\n",
    "        if verbose:\n",
    "            print(f'{t:15s} {label_list[idx]:10s}')\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; БОЕЦ БЕЗ ПРАВИЛ, ТРЕЙЛЕР на русско...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ШОК! КАК ЖЕ ЭТОМУ БОМЖУ ВЕЗЕТ В Br...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ДРЕВНИЕ РОБОТЫ NATASHA TALON И LEO...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ЖЕНА ПУТЕШЕСТВЕННИКА ВО ВРЕМЕНИ = ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Кинозал ДК приглашает на мультфиль...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          video_info  entities_prediction\n",
       "0  <НАЗВАНИЕ:> БОЕЦ БЕЗ ПРАВИЛ, ТРЕЙЛЕР на русско...                  NaN\n",
       "1  <НАЗВАНИЕ:> ШОК! КАК ЖЕ ЭТОМУ БОМЖУ ВЕЗЕТ В Br...                  NaN\n",
       "2  <НАЗВАНИЕ:> ДРЕВНИЕ РОБОТЫ NATASHA TALON И LEO...                  NaN\n",
       "3  <НАЗВАНИЕ:> ЖЕНА ПУТЕШЕСТВЕННИКА ВО ВРЕМЕНИ = ...                  NaN\n",
       "4  <НАЗВАНИЕ:> Кинозал ДК приглашает на мультфиль...                  NaN"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"ner_data_test.csv\")\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1606, 2)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# данные спарсены с Толоки, поэтому могут иметь проблемы с символами и их нужно избежать, \n",
    "# удалить лишние '\\' например, преобразовать из str в список dict-ов\n",
    "# test = data.copy()\n",
    "# test['entities'] = test['entities'].apply(lambda l: l.replace('\\,', ',')if isinstance(l, str) else l)\n",
    "# test['entities'] = test['entities'].apply(lambda l: l.replace('\\\\\\\\', '\\\\')if isinstance(l, str) else l)\n",
    "# test['entities'] = test['entities'].apply(lambda l: '[' + l + ']'if isinstance(l, str) else l)\n",
    "# test['entities'] = test['entities'].apply(lambda l: json.loads(l)if isinstance(l, str) else l)\n",
    "\n",
    "# test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# submission = pd.DataFrame(columns=[['video_info', 'entities_prediction']])\n",
    "# submission['entities_prediction'] = submission['entities_prediction'].astype('object')\n",
    "\n",
    "# def sample_submission(text, tokenizer, model, pipe, submission):\n",
    "#     for i, elem in tqdm(enumerate(ner_test.iloc[:100])):\n",
    "#         labels = predict_ner(elem['tokens'], tokenizer, model, pipe, verbose=False)\n",
    "#         submission.loc[i, 'video_info'] = elem[\"tokens\"]\n",
    "\n",
    "#         submission.loc[i, 'entities_prediction'] = [[label] for label in labels]\n",
    "#     return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1150363ac054614ba884dbb4697e8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1606 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = pd.read_csv(\"sample_submission.csv\")\n",
    "sample_len = sample[\"entities_prediction\"].apply(lambda x: len(eval(x)))\n",
    "\n",
    "for i, elem in tqdm(enumerate(test[\"video_info\"]), total=len(test[\"video_info\"])):\n",
    "    raw_toks = list(tokenize(elem))\n",
    "    words = [tok.text for tok in raw_toks]\n",
    "    \n",
    "    labels = predict_ner(words, tokenizer, model, pipe, verbose=False)\n",
    "    # submission.loc[i, 'video_info'] = elem[\"tokens\"]\n",
    "    \n",
    "    labels = labels[1:-1]\n",
    "        \n",
    "    if len(labels) < sample_len[i]:\n",
    "        amount = sample_len[i] - len(labels)\n",
    "        labels += ['O'] * amount\n",
    "        print(amount)\n",
    "    \n",
    "    labels = labels[:sample_len[i]]\n",
    "    \n",
    "    if len(labels) != sample_len[i]:\n",
    "        print(\"что\")\n",
    "        # labels = labels[:len(words)]\n",
    "\n",
    "    # test.loc[i, 'entities_prediction'] = [[label] for label in labels]\n",
    "    test.loc[i, 'entities_prediction'] = [[label] for label in labels]\n",
    "# return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 112)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([tok.text for tok in list(tokenize(test[\"video_info\"][0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.loc[0][\"entities_prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\t\tO\n",
      "НАЗВАНИЕ\t\tO\n",
      ":\t\tO\n",
      ">\t\tO\n",
      "БОЕЦ\t\tO\n",
      "БЕЗ\t\tO\n",
      "ПРАВИЛ\t\tO\n",
      ",\t\tB-название проекта\n",
      "ТРЕЙЛЕР\t\tB-название проекта\n",
      "на\t\tB-название проекта\n",
      "русском\t\tI-название проекта\n",
      ",\t\tI-название проекта\n",
      "фильм\t\tI-название проекта\n",
      "2021\t\tO\n",
      "|\t\tO\n",
      "боевик\t\tO\n",
      ",\t\tO\n",
      "MMA\t\tO\n",
      "<\t\tO\n",
      "ОПИСАНИЕ\t\tO\n",
      ":\t\tO\n",
      ">\t\tO\n",
      "Больше\t\tO\n",
      "информации\t\tO\n",
      "в\t\tB-Дата\n",
      "нашей\t\tO\n",
      "группе\t\tO\n",
      "ВК\t\tO\n",
      ":\t\tO\n",
      "<\t\tO\n",
      "LINK\t\tO\n",
      ">\t\tO\n",
      "Русский\t\tB-вид спорта\n",
      "трейлер\t\tB-вид спорта\n",
      "фильма\t\tO\n",
      "БОЕЦ\t\tO\n",
      "БЕЗ\t\tO\n",
      "ПРАВИЛ\t\tO\n",
      "2021\t\tO\n",
      "г\t\tO\n",
      "🎬\t\tO\n",
      "Боец\t\tO\n",
      "без\t\tO\n",
      "правил\t\tO\n",
      "(\t\tO\n",
      "2021\t\tO\n",
      ")\t\tO\n",
      "русский\t\tO\n",
      "трейлер\t\tO\n",
      "Notorious\t\tO\n",
      "Nick\t\tO\n",
      "(\t\tO\n",
      "2021\t\tO\n",
      ")\t\tO\n",
      "/\t\tO\n",
      "США\t\tO\n",
      "драма\t\tO\n",
      ",\t\tO\n",
      "боевик\t\tO\n",
      "Дата\t\tO\n",
      "выхода\t\tB-название проекта\n",
      ":\t\tB-название проекта\n",
      "7\t\tB-название проекта\n",
      "августа\t\tI-название проекта\n",
      "2021\t\tI-название проекта\n",
      "г\t\tI-название проекта\n",
      "(\t\tB-Дата\n",
      "мир\t\tI-Дата\n",
      ")\t\tO\n",
      "2\t\tO\n",
      "сентября\t\tB-название проекта\n",
      "2021\t\tB-название проекта\n",
      "г\t\tI-название проекта\n",
      "(\t\tI-название проекта\n",
      "Россия\t\tO\n",
      ")\t\tB-Дата\n",
      "Ник\t\tO\n",
      "Ньюэлл\t\tO\n",
      ",\t\tO\n",
      "однорукий\t\tO\n",
      "боец\t\tO\n",
      "ММА\t\tB-название проекта\n",
      ",\t\tB-название проекта\n",
      "получает\t\tB-название проекта\n",
      "редкую\t\tI-название проекта\n",
      "возможность\t\tO\n",
      "принять\t\tB-Дата\n",
      "участие\t\tO\n",
      "в\t\tO\n",
      "чемпионате\t\tB-локация\n",
      ".\t\tO\n",
      "Герой\t\tO\n",
      "жаждет\t\tO\n",
      "победить\t\tO\n",
      "не\t\tO\n",
      "только\t\tO\n",
      "ради\t\tO\n",
      "себя\t\tO\n",
      ",\t\tO\n",
      "но\t\tB-Дата\n",
      "и\t\tI-Дата\n",
      "ради\t\tI-Дата\n",
      "людей\t\tI-Дата\n",
      "со\t\tO\n",
      "всего\t\tO\n",
      "мира\t\tO\n",
      ",\t\tB-Дата\n",
      "которым\t\tI-Дата\n",
      "знакомы\t\tI-Дата\n",
      "физические\t\tI-Дата\n",
      "трудности\t\tO\n",
      ".\t\tB-локация\n",
      "Режиссер\t\tO\n",
      ":\t\tB-персона\n",
      "Аарон\t\tI-персона\n",
      "Леонг\t\tI-персона\n",
      "Актёры\t\tI-персона\n",
      ":\t\tO\n",
      "Элизабет\t\tO\n",
      "Рём\t\tO\n",
      ",\t\tO\n",
      "Кевин\t\tO\n",
      "Поллак\t\tO\n",
      ",\t\tO\n",
      "Коди\t\tB-вид спорта\n",
      "Кристиан\t\tB-вид спорта\n",
      ",\t\tO\n",
      "Бэрри\t\tO\n",
      "Ливингстон\t\tO\n",
      ",\t\tO\n",
      "Энтони\t\tO\n",
      "Сноу\t\tO\n",
      ",\t\tO\n",
      "Сэмюэл\t\tO\n",
      "Ивэн\t\tO\n",
      "Хоровиц\t\tO\n",
      ",\t\tO\n",
      "Конлан\t\tO\n",
      "Кисилевич\t\tO\n",
      ",\t\tO\n",
      "Марк\t\tO\n",
      "С\t\tO\n",
      ".\t\tO\n",
      "Аллен\t\tO\n",
      ",\t\tO\n",
      "Эрик\t\tO\n",
      "Йорн\t\tO\n",
      "Сундквист\t\tO\n",
      "#\t\tO\n",
      "Боецбезправил\t\tO\n",
      "#\t\tO\n",
      "фильм\t\tO\n",
      "2021\t\tO\n",
      "#\t\tO\n",
      "русскийтрейлер\t\tO\n",
      "#\t\tO\n",
      "боиММА\t\tO\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([f\"{text}\\t\\t{ent}\" for (text, ent) in \n",
    "                 zip([tok.text for tok in list(tokenize(test[\"video_info\"][0]))], \n",
    "                     test.loc[0][\"entities_prediction\"])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [O, O, O, O, O, O, O, B-название проекта, B-на...\n",
       "1       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "2       [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "3       [O, O, O, O, O, O, O, B-название проекта, B-на...\n",
       "4       [O, O, O, O, O, O, O, B-организация, B-организ...\n",
       "                              ...                        \n",
       "1601    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "1602    [O, O, O, O, O, O, O, B-персона, B-персона, I-...\n",
       "1603    [O, O, O, O, O, O, O, O, O, O, O, B-персона, I...\n",
       "1604    [O, O, O, O, O, O, O, B-персона, I-персона, I-...\n",
       "1605    [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...\n",
       "Name: entities_prediction, Length: 1606, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"entities_prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157, 165, 150, 64, 110, 116, 131, 69, 135, 125, 62, 116, 82, 82, 95, 192, 138, 126, 71, 86, 44, 57, 91, 83, 92, 153, 92, 122, 89, 170, 84, 61, 65, 113, 125, 96, 115, 82, 70, 120, 182, 174, 139, 124, 183, 65, 151, 79, 155, 178, 117, 147, 99, 56, 154, 68, 179, 81, 46, 79, 69, 93, 110, 56, 140, 123, 117, 136, 132, 83, 126, 151, 95, 75, 68, 74, 152, 141, 125, 61, 63, 110, 107, 48, 120, 82, 84, 84, 65, 65, 142, 85, 120, 206, 87, 53, 124, 181, 173, 50, 162, 91, 162, 83, 93, 169, 53, 245, 213, 100, 81, 110, 151, 76, 132, 97, 64, 159, 108, 77, 78, 164, 94, 110, 148, 121, 103, 65, 54, 116, 190, 152, 182, 182, 87, 152, 80, 103, 104, 49, 44, 128, 79, 80, 93, 170, 127, 77, 99, 115, 115, 111, 165, 146, 79, 70, 51, 74, 187, 71, 88, 143, 65, 78, 93, 67, 172, 94, 135, 97, 92, 147, 70, 114, 118, 51, 164, 155, 65, 63, 77, 116, 66, 69, 67, 157, 75, 121, 92, 141, 185, 183, 104, 89, 111, 111, 61, 63, 151, 75, 106, 111, 90, 86, 68, 173, 88, 175, 188, 176, 62, 93, 59, 113, 80, 55, 94, 71, 79, 130, 181, 110, 91, 97, 74, 110, 185, 106, 110, 92, 79, 153, 132, 101, 87, 90, 179, 136, 61, 90, 154, 105, 54, 120, 74, 70, 153, 107, 66, 55, 166, 90, 115, 100, 96, 60, 139, 63, 94, 79, 85, 46, 81, 67, 185, 103, 80, 114, 53, 69, 175, 124, 138, 125, 67, 137, 99, 140, 58, 76, 87, 103, 57, 57, 81, 148, 87, 111, 44, 97, 92, 94, 58, 70, 144, 86, 68, 143, 93, 89, 72, 105, 201, 91, 111, 179, 88, 115, 160, 66, 89, 68, 109, 155, 63, 74, 93, 122, 131, 112, 171, 109, 100, 125, 177, 73, 123, 143, 59, 71, 164, 133, 105, 88, 94, 167, 106, 124, 163, 45, 213, 112, 74, 79, 138, 64, 181, 54, 70, 130, 175, 156, 74, 59, 68, 88, 78, 74, 179, 92, 59, 50, 109, 99, 88, 57, 191, 60, 91, 241, 51, 111, 52, 181, 113, 51, 129, 79, 95, 151, 65, 79, 115, 103, 65, 93, 88, 62, 74, 56, 106, 89, 58, 155, 107, 109, 65, 68, 197, 48, 63, 105, 108, 89, 135, 123, 143, 96, 153, 106, 63, 127, 110, 153, 134, 187, 108, 74, 82, 119, 69, 82, 148, 103, 80, 102, 104, 82, 118, 163, 139, 67, 82, 72, 122, 74, 145, 75, 63, 57, 204, 67, 183, 126, 68, 84, 125, 82, 97, 84, 89, 69, 98, 175, 52, 109, 142, 152, 157, 126, 138, 170, 57, 186, 54, 73, 103, 64, 149, 48, 210, 63, 86, 175, 90, 98, 71, 65, 66, 92, 100, 117, 182, 221, 101, 84, 116, 69, 105, 176, 70, 166, 204, 64, 89, 151, 113, 189, 110, 65, 60, 96, 72, 196, 185, 59, 97, 176, 62, 61, 58, 133, 164, 106, 167, 91, 66, 94, 52, 191, 125, 104, 179, 68, 98, 118, 115, 87, 161, 65, 56, 92, 60, 68, 135, 70, 77, 105, 49, 178, 122, 114, 69, 150, 132, 161, 73, 125, 98, 87, 61, 161, 82, 103, 54, 87, 87, 128, 73, 73, 113, 138, 111, 150, 78, 87, 71, 164, 91, 105, 113, 53, 112, 73, 88, 131, 152, 127, 57, 91, 126, 72, 65, 44, 49, 100, 121, 87, 51, 84, 103, 71, 117, 113, 126, 119, 78, 89, 71, 89, 113, 138, 99, 171, 96, 65, 62, 197, 139, 87, 55, 129, 70, 56, 60, 70, 63, 436, 88, 85, 194, 79, 170, 131, 126, 196, 65, 98, 72, 151, 65, 77, 106, 78, 90, 104, 149, 67, 167, 65, 49, 127, 67, 185, 67, 78, 55, 71, 200, 137, 104, 72, 74, 109, 91, 57, 101, 89, 119, 75, 93, 72, 181, 59, 139, 138, 61, 97, 65, 74, 169, 97, 188, 98, 227, 84, 136, 112, 77, 87, 92, 88, 84, 97, 146, 106, 84, 64, 147, 70, 64, 61, 100, 107, 184, 173, 88, 79, 110, 116, 52, 108, 171, 130, 99, 191, 80, 145, 165, 112, 101, 143, 101, 168, 51, 80, 145, 110, 74, 148, 174, 147, 84, 108, 141, 78, 157, 59, 121, 63, 103, 124, 83, 135, 136, 79, 123, 69, 155, 71, 135, 186, 189, 64, 82, 127, 74, 85, 140, 138, 101, 152, 60, 57, 72, 173, 94, 161, 162, 149, 144, 162, 150, 161, 79, 73, 188, 108, 63, 58, 70, 57, 102, 122, 91, 112, 51, 197, 79, 123, 118, 115, 96, 175, 223, 115, 48, 74, 85, 44, 67, 64, 43, 111, 106, 156, 99, 202, 154, 160, 83, 139, 101, 118, 208, 71, 103, 126, 77, 68, 184, 69, 115, 56, 132, 132, 68, 196, 98, 154, 71, 60, 187, 65, 72, 62, 82, 102, 107, 79, 118, 132, 136, 176, 155, 172, 60, 87, 104, 80, 90, 80, 118, 103, 188, 90, 51, 72, 80, 152, 72, 147, 75, 132, 126, 121, 76, 138, 115, 239, 148, 67, 122, 151, 80, 146, 115, 81, 123, 48, 156, 84, 144, 142, 116, 64, 114, 144, 166, 53, 83, 115, 99, 176, 157, 178, 76, 87, 88, 88, 82, 64, 92, 56, 54, 55, 146, 124, 70, 51, 113, 122, 73, 238, 269, 68, 155, 79, 113, 138, 160, 208, 105, 106, 129, 170, 108, 62, 91, 156, 77, 89, 165, 111, 203, 125, 85, 98, 73, 79, 154, 77, 67, 178, 133, 96, 134, 68, 47, 53, 69, 143, 49, 50, 76, 144, 171, 56, 105, 75, 99, 150, 98, 121, 199, 86, 58, 152, 177, 64, 67, 174, 116, 88, 94, 173, 72, 85, 127, 67, 137, 137, 83, 138, 81, 127, 105, 104, 77, 87, 90, 67, 110, 94, 79, 213, 97, 68, 71, 139, 139, 125, 68, 115, 148, 105, 93, 109, 149, 197, 62, 57, 95, 81, 39, 139, 164, 72, 154, 73, 194, 83, 116, 115, 86, 101, 99, 123, 172, 112, 59, 139, 166, 74, 114, 58, 178, 127, 78, 81, 125, 94, 62, 128, 59, 73, 135, 56, 86, 99, 114, 64, 107, 105, 176, 68, 186, 73, 92, 51, 63, 86, 150, 96, 99, 70, 54, 95, 75, 186, 62, 118, 137, 73, 187, 60, 88, 58, 62, 77, 98, 53, 79, 80, 183, 174, 97, 268, 111, 72, 73, 190, 165, 70, 108, 162, 103, 91, 184, 62, 168, 98, 115, 126, 72, 171, 81, 80, 85, 68, 75, 111, 128, 100, 105, 124, 193, 111, 92, 51, 98, 116, 67, 74, 142, 59, 153, 90, 118, 61, 177, 79, 104, 61, 61, 80, 92, 72, 232, 92, 195, 92, 100, 70, 116, 66, 134, 84, 66, 78, 51, 54, 168, 67, 70, 194, 149, 85, 119, 118, 84, 98, 110, 70, 81, 101, 96, 165, 93, 79, 177, 179, 144, 77, 117, 120, 117, 88, 115, 119, 80, 163, 184, 52, 122, 207, 47, 84, 73, 144, 57, 174, 93, 115, 53, 49, 91, 92, 55, 168, 80, 134, 93, 131, 154, 105, 110, 47, 91, 181, 51, 150, 142, 111, 93, 125, 74, 132, 121, 70, 165, 71, 177, 119, 77, 158, 63, 154, 51, 91, 167, 63, 158, 158, 64, 57, 171, 67, 129, 65, 103, 63, 167, 65, 193, 155, 141, 69, 68, 96, 76, 71, 78, 82, 115, 57, 92, 52, 185, 180, 66, 81, 67, 73, 102, 149, 61, 174, 181, 151, 98, 70, 98, 58, 119, 124, 98, 131, 76, 115, 28, 71, 146, 183, 76, 185, 69, 110, 142, 108, 107, 85, 185, 63, 162, 95, 144, 70, 112, 175, 168, 57, 51, 175, 182, 205, 49, 84, 178, 66, 73, 64, 77, 115, 200, 76, 103, 158, 61, 144, 85, 69, 150, 81, 69, 161, 107, 113, 175, 136, 77, 55, 125, 84, 178, 100, 165, 69, 135, 121, 71, 86, 81, 124, 81, 120, 59, 56, 61, 134, 142, 110, 178, 58, 177, 139, 89, 61, 184, 108, 58, 82, 55, 168, 113, 102, 120, 87, 137, 146, 111, 142, 135, 148, 57, 173, 128, 195, 136, 124, 201, 157, 78, 68, 86, 168, 77, 119, 63, 151, 49, 69, 71, 139, 79, 161, 67, 163, 68, 123, 55, 150, 124, 92, 190, 93, 137, 195, 94, 57, 172, 81, 116, 72, 65, 165, 95, 82, 65, 49, 56, 100, 79, 104, 84, 136, 70, 171, 92, 88, 73, 101, 149, 131, 54, 65, 138, 136, 135, 68, 51, 136, 66, 66, 116, 56, 96, 65, 91, 177, 88, 191, 76, 85, 126, 63, 85, 82, 60, 161, 85, 74, 65, 161, 173, 170, 62, 207, 108, 84, 95, 132, 62, 94, 171, 164, 66, 210, 91, 182, 177, 123, 152, 99, 77, 100, 109, 137, 67, 173, 65, 158, 116, 64, 193, 57, 49, 87, 135, 61, 167, 124, 46, 53, 66, 56, 123, 171, 108, 80, 57, 81, 148, 110, 128, 76, 135, 100, 80, 81, 61, 84, 100, 56, 40, 83, 110, 103, 87, 71, 119, 135, 114, 132, 151, 111, 128, 89, 190, 137, 139, 73, 128, 180, 131, 270, 141, 166, 60, 69, 53, 147, 90, 144, 156, 108, 150, 43, 181, 168, 192, 65, 128, 154, 112, 134, 211, 95, 101, 143, 112, 177, 84, 68, 158, 67, 60, 108, 84, 64, 160, 109, 103, 84, 128, 126, 114, 158, 67, 66, 132, 52, 96, 109, 98, 184, 119, 91, 151, 66, 182, 77, 90, 90, 62, 64, 97, 135, 143, 80, 66, 143, 57, 91, 55, 112]\n"
     ]
    }
   ],
   "source": [
    "print(test[\"entities_prediction\"].apply(len).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; БОЕЦ БЕЗ ПРАВИЛ, ТРЕЙЛЕР на русско...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-название проекта, B-на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ШОК! КАК ЖЕ ЭТОМУ БОМЖУ ВЕЗЕТ В Br...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ДРЕВНИЕ РОБОТЫ NATASHA TALON И LEO...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ЖЕНА ПУТЕШЕСТВЕННИКА ВО ВРЕМЕНИ = ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-название проекта, B-на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Кинозал ДК приглашает на мультфиль...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-организация, B-организ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Милейшие детеныши животных, которы...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Диана Шурыгина набросилась на зрит...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-персона, B-персона, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Губернатор Евгений Куйвашев и непо...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-персона, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Александр Горелов = о вакцинации о...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-персона, I-персона, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Смотрела УЖАСТИК и забыла, что не ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1606 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             video_info  \\\n",
       "0     <НАЗВАНИЕ:> БОЕЦ БЕЗ ПРАВИЛ, ТРЕЙЛЕР на русско...   \n",
       "1     <НАЗВАНИЕ:> ШОК! КАК ЖЕ ЭТОМУ БОМЖУ ВЕЗЕТ В Br...   \n",
       "2     <НАЗВАНИЕ:> ДРЕВНИЕ РОБОТЫ NATASHA TALON И LEO...   \n",
       "3     <НАЗВАНИЕ:> ЖЕНА ПУТЕШЕСТВЕННИКА ВО ВРЕМЕНИ = ...   \n",
       "4     <НАЗВАНИЕ:> Кинозал ДК приглашает на мультфиль...   \n",
       "...                                                 ...   \n",
       "1601  <НАЗВАНИЕ:> Милейшие детеныши животных, которы...   \n",
       "1602  <НАЗВАНИЕ:> Диана Шурыгина набросилась на зрит...   \n",
       "1603  <НАЗВАНИЕ:> Губернатор Евгений Куйвашев и непо...   \n",
       "1604  <НАЗВАНИЕ:> Александр Горелов = о вакцинации о...   \n",
       "1605  <НАЗВАНИЕ:> Смотрела УЖАСТИК и забыла, что не ...   \n",
       "\n",
       "                                    entities_prediction  \n",
       "0     [O, O, O, O, O, O, O, B-название проекта, B-на...  \n",
       "1     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3     [O, O, O, O, O, O, O, B-название проекта, B-на...  \n",
       "4     [O, O, O, O, O, O, O, B-организация, B-организ...  \n",
       "...                                                 ...  \n",
       "1601  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1602  [O, O, O, O, O, O, O, B-персона, B-персона, I-...  \n",
       "1603  [O, O, O, O, O, O, O, O, O, O, O, B-персона, I...  \n",
       "1604  [O, O, O, O, O, O, O, B-персона, I-персона, I-...  \n",
       "1605  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "\n",
       "[1606 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"submit_xlm_roberta_v1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; БОЕЦ БЕЗ ПРАВИЛ, ТРЕЙЛЕР на русско...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-названи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ШОК! КАК ЖЕ ЭТОМУ БОМЖУ ВЕЗЕТ В Br...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ДРЕВНИЕ РОБОТЫ NATASHA TALON И LEO...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ЖЕНА ПУТЕШЕСТВЕННИКА ВО ВРЕМЕНИ = ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-названи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Кинозал ДК приглашает на мультфиль...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-организ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Милейшие детеныши животных, которы...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Диана Шурыгина набросилась на зрит...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-персона...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Губернатор Евгений Куйвашев и непо...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Александр Горелов = о вакцинации о...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-персона...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Смотрела УЖАСТИК и забыла, что не ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1606 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             video_info  \\\n",
       "0     <НАЗВАНИЕ:> БОЕЦ БЕЗ ПРАВИЛ, ТРЕЙЛЕР на русско...   \n",
       "1     <НАЗВАНИЕ:> ШОК! КАК ЖЕ ЭТОМУ БОМЖУ ВЕЗЕТ В Br...   \n",
       "2     <НАЗВАНИЕ:> ДРЕВНИЕ РОБОТЫ NATASHA TALON И LEO...   \n",
       "3     <НАЗВАНИЕ:> ЖЕНА ПУТЕШЕСТВЕННИКА ВО ВРЕМЕНИ = ...   \n",
       "4     <НАЗВАНИЕ:> Кинозал ДК приглашает на мультфиль...   \n",
       "...                                                 ...   \n",
       "1601  <НАЗВАНИЕ:> Милейшие детеныши животных, которы...   \n",
       "1602  <НАЗВАНИЕ:> Диана Шурыгина набросилась на зрит...   \n",
       "1603  <НАЗВАНИЕ:> Губернатор Евгений Куйвашев и непо...   \n",
       "1604  <НАЗВАНИЕ:> Александр Горелов = о вакцинации о...   \n",
       "1605  <НАЗВАНИЕ:> Смотрела УЖАСТИК и забыла, что не ...   \n",
       "\n",
       "                                    entities_prediction  \n",
       "0     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-названи...  \n",
       "1     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "2     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "3     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-названи...  \n",
       "4     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-организ...  \n",
       "...                                                 ...  \n",
       "1601  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1602  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-персона...  \n",
       "1603  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1604  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-персона...  \n",
       "1605  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "\n",
       "[1606 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"submit_xlm_roberta_v1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_info</th>\n",
       "      <th>entities_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; БОЕЦ БЕЗ ПРАВИЛ, ТРЕЙЛЕР на русско...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ШОК! КАК ЖЕ ЭТОМУ БОМЖУ ВЕЗЕТ В Br...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ДРЕВНИЕ РОБОТЫ NATASHA TALON И LEO...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; ЖЕНА ПУТЕШЕСТВЕННИКА ВО ВРЕМЕНИ = ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Кинозал ДК приглашает на мультфиль...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1601</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Милейшие детеныши животных, которы...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Диана Шурыгина набросилась на зрит...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Губернатор Евгений Куйвашев и непо...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Александр Горелов = о вакцинации о...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>&lt;НАЗВАНИЕ:&gt; Смотрела УЖАСТИК и забыла, что не ...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1606 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             video_info  \\\n",
       "0     <НАЗВАНИЕ:> БОЕЦ БЕЗ ПРАВИЛ, ТРЕЙЛЕР на русско...   \n",
       "1     <НАЗВАНИЕ:> ШОК! КАК ЖЕ ЭТОМУ БОМЖУ ВЕЗЕТ В Br...   \n",
       "2     <НАЗВАНИЕ:> ДРЕВНИЕ РОБОТЫ NATASHA TALON И LEO...   \n",
       "3     <НАЗВАНИЕ:> ЖЕНА ПУТЕШЕСТВЕННИКА ВО ВРЕМЕНИ = ...   \n",
       "4     <НАЗВАНИЕ:> Кинозал ДК приглашает на мультфиль...   \n",
       "...                                                 ...   \n",
       "1601  <НАЗВАНИЕ:> Милейшие детеныши животных, которы...   \n",
       "1602  <НАЗВАНИЕ:> Диана Шурыгина набросилась на зрит...   \n",
       "1603  <НАЗВАНИЕ:> Губернатор Евгений Куйвашев и непо...   \n",
       "1604  <НАЗВАНИЕ:> Александр Горелов = о вакцинации о...   \n",
       "1605  <НАЗВАНИЕ:> Смотрела УЖАСТИК и забыла, что не ...   \n",
       "\n",
       "                                    entities_prediction  \n",
       "0     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "2     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "3     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "4     ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "...                                                 ...  \n",
       "1601  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1602  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1603  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1604  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1605  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "\n",
       "[1606 rows x 2 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"sample_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
